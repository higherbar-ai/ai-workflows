{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/higherbar-ai/ai-workflows/blob/main/src/example-doc-extraction.ipynb\" target=\"_parent\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>\n",
    "\n",
    "# About this notebook\n",
    "\n",
    "This notebook provides an example of how the `ai-workflows` package can be used to extract structured data from an arbitrary number of unstructured documents. It is set up to extract questions from survey forms, but it is designed to be modified to extract any kind of structured data from any kind of document.\n",
    "\n",
    "If you'd rather not edit the code in this notebook to adapt it to your needs, you can use [the generic version of this notebook that extracts structured data based on an Excel template](https://github.com/higherbar-ai/ai-workflows/blob/main/src/example-doc-extraction-templated.ipynb).\n",
    "\n",
    "## Configuration\n",
    "\n",
    "This notebook requires different settings depending on which AI service providers you want to use. If you're running in Google Colab, you configure these settings as \"secrets\"; just click the key icon in the left sidebar (and, once you create a secret, be sure to click the toggle to give the notebook access to the secret). If you're running this notebook in a different environment, you can set these settings in a `.env` file; the first time you run, it will write out a template `.env` file for you to fill in and direct you to its location. Following are the settings, regardless of the environment.\n",
    "\n",
    "If you don't have an API key for an AI provider yet, [see here to learn what that is and how to get one](https://www.linkedin.com/pulse/those-genai-api-keys-christopher-robert-l5rie/).\n",
    "\n",
    "### OpenAI (direct)\n",
    "\n",
    "To use OpenAI directly:\n",
    "\n",
    "* `openai_api_key` - your OpenAI API key (get one from [the OpenAI API key page](https://platform.openai.com/api-keys), and be sure to fund your platform account with at least $5 to allow GPT-4o model access)\n",
    "* `openai_model` (optional) - the model to use (defaults to `gpt-4o`)\n",
    "\n",
    "### OpenAI (via Microsoft Azure)\n",
    "\n",
    "To use OpenAI via Microsoft Azure:\n",
    "\n",
    "* `azure_api_key` - your Azure API key\n",
    "* `azure_api_base` - the base URL for the Azure API\n",
    "* `azure_api_engine` - the engine to use (a.k.a. the \"deployment\")\n",
    "* `azure_api_version` - the API version to use\n",
    "\n",
    "### Anthropic (direct)\n",
    "\n",
    "To use Anthropic directly:\n",
    "\n",
    "* `anthropic_api_key` - your Anthropic API key\n",
    "* `anthropic_model` - the model to use\n",
    "\n",
    "### LangSmith (for tracing)\n",
    "\n",
    "Optionally, you can add [LangSmith tracing](https://langchain.com/langsmith):\n",
    "\n",
    "* `langsmith_api_key` - your LangSmith API key\n",
    "\n",
    "## Setting up the runtime environment\n",
    "\n",
    "This next code block installs all necessary Python and system packages into the current environment.\n",
    "\n",
    "**If you're running in Google Colab and it prompts you to restart the notebook in the middle of the installation steps, just click CANCEL.**"
   ],
   "id": "3131c033cc6ec753"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# install Google Colab Support and ai_workflows package\n",
    "%pip install colab-or-not py-ai-workflows[docs]\n",
    "\n",
    "# download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', force=True)\n",
    "\n",
    "# set up our notebook environment (including LibreOffice)\n",
    "from colab_or_not import NotebookBridge\n",
    "notebook_env = NotebookBridge(\n",
    "    system_packages=[\"libreoffice\"],\n",
    "    config_path=\"~/.hbai/ai-workflows.env\",\n",
    "    config_template={\n",
    "        \"openai_api_key\": \"\",\n",
    "        \"openai_model\": \"\",\n",
    "        \"azure_api_key\": \"\",\n",
    "        \"azure_api_base\": \"\",\n",
    "        \"azure_api_engine\": \"\",\n",
    "        \"azure_api_version\": \"\",\n",
    "        \"anthropic_api_key\": \"\",\n",
    "        \"anthropic_model\": \"\",\n",
    "        \"langsmith_api_key\": \"\",\n",
    "    }\n",
    ")\n",
    "notebook_env.setup_environment()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuring the document extraction job\n",
    "\n",
    "This next code block provides the configuration necessary to guide the document extraction process.\n",
    "\n",
    "**You'll want to modify this section to meet your specific needs.**"
   ],
   "id": "bde1bca40b01788a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TO-DO: edit the context below to provide a description for each document that you will be processing\n",
    "json_context = \"\"\"The file contains the following:\n",
    "\n",
    "A survey instrument or digital form\"\"\"\n",
    "\n",
    "# TO-DO: edit the job text below to provide a description of each row that you want in your extracted data output\n",
    "json_job = \"\"\"Your job is to extract a series of row objects from the file's content, and to return them all in a specific JSON format. Each row object should represent the following:\n",
    "\n",
    "An individual question or field included in the survey or digital form (with each translation of a single question or field being treated as a separate row)\"\"\"\n",
    "\n",
    "# TO-DO: edit the specification below to customize the list of fields that you want included in each row (these will be columns in your output data)\n",
    "json_output_spec = \"\"\"Return JSON with the following fields (and only the following fields):\n",
    "\n",
    "* `rows` (list): The list of rows extracted, or an empty list if none found. Each row should contain the following fields:\n",
    "\n",
    "  * `question_id` (string): The numeric or alphanumeric identifier or short variable name identifying the question (if any), usually located just before or at the beginning of the question. \"\" if none found.\n",
    "\n",
    "  * `question` (string): The exact text of the question or form field, including any introductory text that provides context or explanation. Often follows a unique question ID of some sort, like \"2.01.\" or \"gender:\". Should not include response options, which should be included in the 'options' field, or extra enumerator or interviewer instructions (including interview probes), which should be included in the 'instructions' field. Be careful: the same question might be asked in multiple languages, and each translation should be included as a separate row. Never translate between languages or otherwise alter the question text in any way.\n",
    "\n",
    "  * `instructions` (string): Instructions or other guidance about how to ask or answer the question (if any), including enumerator or interviewer instructions. If the question includes a list of specific response options, do NOT include those in the instructions. However, if there is guidance as to how to fill out an open-ended numeric or text response, or guidance about how to choose among the options, include that guidance here. \"\" if none found.\n",
    "\n",
    "  * `options` (string): The list of specific response options for multiple-choice questions in a single string, including both the label and the internal value (if specified) for each option. For example, a 'Male' label might be coupled with an internal value of '1', 'M', or even 'male'. Separate response options with a space, three pipe symbols ('|||'), and another space, and, if there is an internal value, add a space, three # symbols ('###'), and the internal value at the end of the label. For example: 'Male ### 1 ||| Female ### 2' (codes included) or 'Male ||| Female' (no codes); 'Yes ### yes ||| No ### no', 'Yes ### 1 ||| No ### 0', 'Yes ### y ||| No ### n', or 'YES ||| NO'. Do NOT include fill-in-the-blank content here, only multiple-choice options. \"\" if the question is open-ended (i.e., does not include specific multiple-choice options).\"\"\"\n",
    "\n",
    "# TO-DO (ADVANCED): if information that should be extracted into a single row never spans multiple pages, you can change the following to False in order to process PDF files page-by-page rather than converting them to Markdown format first\n",
    "markdown_first = True"
   ],
   "id": "5ce6cf5ee9346d11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initializing for AI workflows\n",
    "\n",
    "The next code block initializes the notebook by loading settings and initializing the LLM interface."
   ],
   "id": "343a69892d52b765"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ai_workflows.llm_utilities import LLMInterface\n",
    "from ai_workflows.document_utilities import DocumentInterface\n",
    "\n",
    "# read all supported settings\n",
    "openai_api_key = notebook_env.get_setting('openai_api_key')\n",
    "openai_model = notebook_env.get_setting('openai_model', 'gpt-4o')\n",
    "azure_api_key = notebook_env.get_setting('azure_api_key')\n",
    "azure_api_base = notebook_env.get_setting('azure_api_base')\n",
    "azure_api_engine = notebook_env.get_setting('azure_api_engine')\n",
    "azure_api_version = notebook_env.get_setting('azure_api_version')\n",
    "anthropic_api_key = notebook_env.get_setting(\"anthropic_api_key\")\n",
    "anthropic_model = notebook_env.get_setting(\"anthropic_model\")\n",
    "langsmith_api_key = notebook_env.get_setting('langsmith_api_key')\n",
    "\n",
    "# complain if we don't have the bare minimum to run\n",
    "if (not openai_api_key\n",
    "        and not (azure_api_key and azure_api_base and azure_api_engine and azure_api_version)\n",
    "        and not (anthropic_api_key and anthropic_model)):\n",
    "    raise Exception('We need settings set for OpenAI access (direct or via Azure) or for Anthropic access (direct). See the instructions above for more details.')\n",
    "\n",
    "# initialize LLM interface\n",
    "llm = LLMInterface(openai_api_key=openai_api_key, openai_model=openai_model, azure_api_key=azure_api_key, azure_api_base=azure_api_base, azure_api_engine=azure_api_engine, azure_api_version=azure_api_version, temperature = 0.0, total_response_timeout_seconds=600, number_of_retries=2, seconds_between_retries=5, langsmith_api_key=langsmith_api_key, anthropic_api_key=anthropic_api_key, anthropic_model=anthropic_model)\n",
    "\n",
    "# initialize our document processor\n",
    "doc_interface = DocumentInterface(llm_interface=llm)\n",
    "\n",
    "# report success\n",
    "print(\"Initialization successful.\")"
   ],
   "id": "80a6929f49e358bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompting for input files\n",
    "\n",
    "This next code block prompts you to upload or select the document(s) you want to process. If you only want to process a single document, just upload or select that. If you want to process multiple, upload or select them all or compress them together into a single `.zip` file and upload or select that."
   ],
   "id": "94ead9d4f72c8368"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# prompt for one or more files to process\n",
    "files_to_process = notebook_env.get_input_files(\"Document(s) to process (.zip file for multiple):\")\n",
    "\n",
    "# report out on the files we plan to process\n",
    "for file_to_process in files_to_process:\n",
    "    if file_to_process.lower().endswith('.zip'):\n",
    "        print(f'Will process all files within: {file_to_process}')\n",
    "    else:\n",
    "        print(f'Will process: {file_to_process}')"
   ],
   "id": "bdae454b5dc58584",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extracting data\n",
    "\n",
    "This next code block processes each file one-by-one, unzipping `.zip` files into a temporary directory as needed, and aggregates all the results into a single list of rows."
   ],
   "id": "81b4ca99048e30b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tempfile\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# next, process files, with zip files unzipped into a temporary directory\n",
    "all_rows = []\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # tally up all files, unzipping as needed\n",
    "    all_files = []\n",
    "    for file_to_process in files_to_process:\n",
    "        if file_to_process.lower().endswith('.zip'):\n",
    "            # if it's a .zip file, unzip it into the temporary directory\n",
    "            print(f'Unzipping {file_to_process}')\n",
    "            with zipfile.ZipFile(file_to_process, 'r') as zip_ref:\n",
    "                zip_ref.extractall(temp_dir)\n",
    "        else:\n",
    "            # just add the file to the list of files to process\n",
    "            all_files.append(file_to_process)\n",
    "    # add all unzipped files to the list of files to process (ignoring hidden files)\n",
    "    for root, dirs, files in os.walk(temp_dir):\n",
    "        for unzipped_file in files:\n",
    "            unzipped_file_path = os.path.join(root, unzipped_file)\n",
    "            if not unzipped_file.startswith('.'):\n",
    "                all_files.append(unzipped_file_path)\n",
    "\n",
    "    # process each file\n",
    "    for file_to_process in all_files:\n",
    "        filename = os.path.basename(file_to_process)\n",
    "        print(f'Processing {filename}...')\n",
    "\n",
    "        # process the file\n",
    "        all_responses = doc_interface.convert_to_json(file_to_process, json_context, json_job, json_output_spec, markdown_first=markdown_first)\n",
    "\n",
    "        # combine all responses into a single list of rows\n",
    "        merged_responses = doc_interface.merge_dicts(all_responses)\n",
    "        rows = merged_responses['rows']\n",
    "\n",
    "        # output and save results\n",
    "        print(f\"  Extracted {len(rows)} row{'s' if len(rows) != 1 else ''}\")\n",
    "        all_rows.append((filename, rows))"
   ],
   "id": "6ff4e0f917b555cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Outputting extracted data\n",
    "\n",
    "This final code block outputs the extracted data to `extracted_data.csv`, with the filename in column A and the extracted data columns thereafter.\n",
    "\n",
    "If you're running in Google Colab, this `.csv` file will be saved into the content folder. Find, view, or download it by clicking on the folder icon in the left sidebar.\n",
    "\n",
    "If you're running elsewhere, it will be saved into an `ai-workflows` subdirectory created off of your user home directory."
   ],
   "id": "29bf70b43f324e62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "# output files to ~/ai-workflows directory if local, otherwise /content if Google Colab\n",
    "output_path_prefix = notebook_env.get_output_dir(not_colab_dir=\"~/ai-workflows\", colab_subdir=\"\")\n",
    "\n",
    "# assemble all unique keys in all_rows into a single list\n",
    "data_columns = []\n",
    "for _, rows in all_rows:\n",
    "    for row in rows:\n",
    "        for k in row.keys():\n",
    "            if not k in data_columns:\n",
    "                data_columns.append(k)\n",
    "\n",
    "# output .csv file with extracted data, with filename in column A and the output columns thereafter\n",
    "output_csv_path = os.path.join(output_path_prefix, 'extracted_data.csv')\n",
    "with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['filename'] + data_columns)\n",
    "    for filename, rows in all_rows:\n",
    "        for row in rows:\n",
    "            csvwriter.writerow([filename] + [row.get(k, '') for k in data_columns])\n",
    "\n",
    "# report out on the output file\n",
    "print(f'Extracted data saved to: {output_csv_path}')"
   ],
   "id": "79a0149bcb008083",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
