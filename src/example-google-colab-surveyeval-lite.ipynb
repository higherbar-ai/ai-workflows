{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyPxP3kjKB/kQGaabIV5w0tX",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": "<a href=\"https://colab.research.google.com/github/higherbar-ai/ai-workflows/blob/main/src/example-google-colab-surveyeval-lite.ipynb\" target=\"_parent\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# About this surveyeval-lite notebook (Google Colab version)\n",
    "\n",
    "This notebook provides a simple example of an automated AI workflow. It's a much-simplified version of [the surveyeval toolkit available here in GitHub](https://github.com/higherbar-ai/survey-eval) designed to run in [Google Colab](https://colab.research.google.com). This version, self-contained in this single notebook, uses [the ai-workflows package](https://github.com/higherbar-ai/ai-workflows), along with an OpenAI LLM, to:\n",
    "\n",
    "1. Parse a survey file into a series of questions\n",
    "\n",
    "2. Loop through each question to:\n",
    "\n",
    "    1. Evaluate the question for potential phrasing issues\n",
    "    2. Evaluate the question for potential bias issues\n",
    "\n",
    "3. Assemble and output all findings and recommendations\n",
    "\n",
    "See [the ai-workflows GitHub repo](https://github.com/higherbar-ai/ai-workflows) for more details.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "This notebook uses secrets, which you can configure by clicking on the key icon in Google Colab's left sidebar.\n",
    "\n",
    "To use OpenAI directly, configure the `openai_api_key` secret to contain your API key. (Get a key from [the OpenAI API key page](https://platform.openai.com/api-keys), and be sure to fund your platform account with at least $5 to allow GPT-4o model access.)\n",
    "\n",
    "Alternatively, you can use OpenAI via Microsoft Azure by configuring the following secrets:\n",
    "\n",
    "1. `azure_api_key`\n",
    "2. `azure_api_base`\n",
    "3. `azure_api_engine`\n",
    "4. `azure_api_version`\n",
    "\n",
    "Finally, you can override the default model of `gpt-4o` by setting the `openai_model` secret to your preferred model, and you can optionally add [LangSmith tracing](https://langchain.com/langsmith) by setting the `langsmith_api_key` secret.\n",
    "\n",
    "## Your survey file\n",
    "\n",
    "This notebook will prompt you to upload a survey file. It will then parse that file into a series of questions and evaluate each question for phrasing and bias issues. The simpler the file's formatting, the better the results will be."
   ],
   "metadata": {
    "id": "IazipVYi_yUy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installing prerequisites\n",
    "\n",
    "This next code block installs all necessary Python packages and system support functions into the current environment.\n",
    "\n",
    "If Google Colab prompts you to restart the notebook in the middle of the installation steps, you can safely cancel it."
   ],
   "metadata": {
    "id": "qZuE5AxQScij"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install google-colab\n",
    "!pip install py-ai-workflows\n",
    "\n",
    "# download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', force=True)\n",
    "\n",
    "# install LibreOffice for document processing\n",
    "!apt-get install -y libreoffice"
   ],
   "metadata": {
    "collapsed": true,
    "id": "UGxo3IreHmUZ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initializing for AI workflows\n",
    "\n",
    "This next code block uses your configured secrets to initialize an AI workflow support package."
   ],
   "metadata": {
    "id": "SGJyVWA1Ua-v"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j9coSno4_Arf"
   },
   "source": [
    "from google.colab import userdata\n",
    "from ai_workflows.llm_utilities import LLMInterface\n",
    "from ai_workflows.document_utilities import DocumentInterface\n",
    "\n",
    "# utility function: get secret from Google Colab, with support for a default\n",
    "def get_secret_with_default(secretName, defaultValue=None):\n",
    "    # noinspection PyBroadException\n",
    "    try:\n",
    "        return userdata.get(secretName)\n",
    "    except:\n",
    "        return defaultValue\n",
    "\n",
    "\n",
    "# read all supported secrets\n",
    "openai_api_key = get_secret_with_default('openai_api_key')\n",
    "openai_model = get_secret_with_default('openai_model', 'gpt-4o')\n",
    "azure_api_key = get_secret_with_default('azure_api_key')\n",
    "azure_api_base = get_secret_with_default('azure_api_base')\n",
    "azure_api_engine = get_secret_with_default('azure_api_engine')\n",
    "azure_api_version = get_secret_with_default('azure_api_version')\n",
    "langsmith_api_key = get_secret_with_default('langsmith_api_key')\n",
    "\n",
    "# complain if we don't have the bare minimum to run\n",
    "if not openai_api_key and not (azure_api_key\n",
    "                               and azure_api_base\n",
    "                               and azure_api_engine\n",
    "                               and azure_api_version):\n",
    "    raise Exception('We need either an openai_api_key secret set in the secrets â€” or set azure_api_key, azure_api_base, azure_api_engine, and azure_api_version to use Azure instead. Also be sure to enable Notebook Access for the secret(s).')\n",
    "\n",
    "# initialize LLM interface\n",
    "llm = LLMInterface(openai_api_key=openai_api_key, openai_model=openai_model, azure_api_key=azure_api_key, azure_api_base=azure_api_base, azure_api_engine=azure_api_engine, azure_api_version=azure_api_version, temperature = 0.0, total_response_timeout_seconds=600, number_of_retries=2, seconds_between_retries=5, langsmith_api_key=langsmith_api_key)\n",
    "\n",
    "# initialize our document processor\n",
    "doc_interface = DocumentInterface(llm_interface=llm)\n",
    "\n",
    "# set Google Colab content directory\n",
    "content_dir = \"/content\"\n",
    "\n",
    "# report success\n",
    "print(\"Initialization successful.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Uploading your survey file and converting it to Markdown\n",
    "\n",
    "When you run this next code cell, it will prompt you to upload a single survey file and use that for the survey evaluation, converting it to Markdown format and outputting the converted text so that you can verify that it looks okay.\n",
    "\n",
    "If you don't have a survey file handy, you can use [this short excerpt from the DHS](https://github.com/higherbar-ai/ai-workflows/blob/main/resources/sample_dhs_questions.txt)."
   ],
   "metadata": {
    "id": "F8gvZYWeSpPD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "from ai_workflows.document_utilities import DocumentInterface\n",
    "\n",
    "# prompt for a single file and keep prompting till we get one\n",
    "print('Upload a file with your survey instrument:')\n",
    "print()\n",
    "while True:\n",
    "    # prompt for upload\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    # complain if we didn't get just a single file\n",
    "    if len(uploaded.items()) != 1:\n",
    "        print()\n",
    "        print('Please upload a single .txt file.')\n",
    "        print()\n",
    "    else:\n",
    "        # fetch the full path of the uploaded file\n",
    "        file_path = os.path.join(content_dir, next(iter(uploaded)))\n",
    "        \n",
    "        # break from loop\n",
    "        break\n",
    "\n",
    "# extract file text as Markdown\n",
    "doc_processor = DocumentInterface(llm_interface=llm)\n",
    "survey_text = doc_processor.convert_to_markdown(file_path)\n",
    "\n",
    "# report results\n",
    "print(f\"Processing this survey file: {file_path}\")\n",
    "print()\n",
    "print(f\"Extracted Markdown text: {survey_text}\")"
   ],
   "metadata": {
    "collapsed": true,
    "id": "u0BAoiTHIJnd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parsing the survey text\n",
    "\n",
    "The next code block will use the LLM to parse the survey text into a list of questions."
   ],
   "metadata": {
    "id": "elA3S_P-9_MU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# configure our parameters for JSON conversion\n",
    "\n",
    "json_context = \"The file contains a survey instrument or digital form.\"\n",
    "\n",
    "json_job = f\"\"\"Your job is to extract survey questions or form fields from the file's content, including question IDs, instructions, and multiple-choice options, and to return it all in a specific JSON format. More specifically:\n",
    "\n",
    "* **Your job is to extract verbatim text:** In the JSON you return, only ever include text content, directly quoted without modification, from the survey text you are supplied (i.e., never add or invent any text and never revise or rephrase any text).\n",
    "\n",
    "* **Only respond with valid JSON that precisely follows the format specified below:** Your response should only include valid JSON and nothing else; if you cannot find any questions to return, simply return an empty questions list.\n",
    "\n",
    "* **Treat translations as separate questions:** If you see one or more translated versions of a question, include them as separate questions in the JSON you return.\"\"\"\n",
    "\n",
    "json_output_spec = f\"\"\"Return JSON with the following fields (and only the following fields):\n",
    "\n",
    "* `questions` (list): The list of questions extracted, or an empty list if none found. Each question should be a dictionary with the following keys:\n",
    "\n",
    "  * `question_id` (string): The numeric or alphanumeric identifier or short variable name identifying the question (if any), usually located just before or at the beginning of the question. \"\" if none found.\n",
    "\n",
    "  * `question` (string): The exact text of the question or form field, including any introductory text that provides context or explanation. Often follows a unique question ID of some sort, like \"2.01.\" or \"gender:\". Should not include response options, which should be included in the 'options' field, or extra enumerator or interviewer instructions (including interview probes), which should be included in the 'instructions' field. Be careful: the same question might be asked in multiple languages, and each translation should be included as a separate question. Never translate between languages or otherwise alter the question text in any way.\n",
    "\n",
    "  * `instructions` (string): Instructions or other guidance about how to ask or answer the question (if any), including enumerator or interviewer instructions. If the question includes a list of specific response options, do NOT include those in the instructions. However, if there is guidance as to how to fill out an open-ended numeric or text response, or guidance about how to choose among the options, include that guidance here. \"\" if none found.\n",
    "\n",
    "  * `options` (string): The list of specific response options for multiple-choice questions in a single string, including both the label and the internal value (if specified) for each option. For example, a 'Male' label might be coupled with an internal value of '1', 'M', or even 'male'. Separate response options with a space, three pipe symbols ('|||'), and another space, and, if there is an internal value, add a space, three # symbols ('###'), and the internal value at the end of the label. For example: 'Male ### 1 ||| Female ### 2' (codes included) or 'Male ||| Female' (no codes); 'Yes ### yes ||| No ### no', 'Yes ### 1 ||| No ### 0', 'Yes ### y ||| No ### n', or 'YES ||| NO'. Do NOT include fill-in-the-blank content here, only multiple-choice options. \"\" if the question is open-ended (i.e., does not include specific multiple-choice options).\"\"\"\n",
    "\n",
    "# process the file\n",
    "all_responses = doc_processor.markdown_to_json(survey_text, json_context, json_job, json_output_spec)\n",
    "\n",
    "# combine all responses into a single list of questions\n",
    "questions = []\n",
    "for response in all_responses:\n",
    "    if 'questions' in response:\n",
    "        questions += response['questions']\n",
    "\n",
    "# output results\n",
    "if questions:\n",
    "    # output summary of results\n",
    "    num_questions = len(questions)\n",
    "    num_question_ids = len(set([q['question_id'] for q in questions]))\n",
    "    num_instructions = len(set([q['instructions'] for q in questions]))\n",
    "    num_options = len(set([q['options'] for q in questions]))\n",
    "    print(f\"Parsed {num_questions} questions ({num_question_ids} with IDs, {num_instructions} with instructions, and {num_options} with multiple-choice options)\")\n",
    "else:\n",
    "    print(f\"Failed to parse any questions from file.\")"
   ],
   "metadata": {
    "id": "DESt6PAj-WKh"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reviewing the survey questions\n",
    "\n",
    "This next code block will review each question in the survey, asking the LLM for advice re: question phrasing as well as potential biased or stereotypical language."
   ],
   "metadata": {
    "id": "CQLAfiV8MlXM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# loop through every question, reviewing them and saving results as we go\n",
    "all_results = []\n",
    "for question in questions:\n",
    "    # format our question for the LLM\n",
    "    question_text = f\"\"\"* Question ID: {question['question_id']}\n",
    "* Instructions: {question['instructions']}\n",
    "* Question: {question['question']}\n",
    "* Options: {question['options']}\"\"\"\n",
    "\n",
    "    # set up our phrasing-review prompt for the LLM\n",
    "    phrasing_prompt = f\"\"\"You are an AI designed to evaluate questionnaires and other survey instruments used by researchers and M&E professionals. You are an expert in survey methodology with training equivalent to a member of the American Association for Public Opinion Research (AAPOR) with a Ph.D. in survey methodology from University of Michiganâ€™s Institute for Social Research. You consider primarily the content, context, and questions provided to you, and then content and methods from the most widely-cited academic publications and public and nonprofit research organizations.\n",
    "\n",
    "You always give truthful, factual answers. When asked to give your response in a specific format, you always give your answer in the exact format requested. You never give offensive responses. If you donâ€™t know the answer to a question, you truthfully say you donâ€™t know.\n",
    "\n",
    "You will be given the raw text from a questionnaire or survey instrument between |!| and |!| delimiters. You will also be given a specific question from that text to evaluate between |@| and |@| delimiters. The question will be supplied in the following format:\n",
    "\n",
    "* Question ID: ID (if any)\n",
    "* Instructions: Instructions (if any)\n",
    "* Question: Question text\n",
    "* Options: Multiple-choice options (if any), with each separated by three pipe symbols (|||) and option values (if any) separated from option labels by three hash symbols (###)\n",
    "\n",
    "Evaluate the question only, but also consider its context within the larger survey.\n",
    "\n",
    "Assume that this survey will be administered by a trained enumerator who asks each question and reads each prompt or instruction as indicated in the excerpt. Your job is to anticipate the phrasing or translation issues that would be identified in a rigorous process of pre-testing (with cognitive interviewing) and piloting.\n",
    "\n",
    "When evaluating the question, DO:\n",
    "\n",
    "1. Ensure that the question will be understandable by substantially all respondents.\n",
    "\n",
    "2. Consider the question in the context of the excerpt, including any instructions, related questions, or prompts that precede it.\n",
    "\n",
    "3. Ignore question numbers and formatting.\n",
    "\n",
    "4. Assume that code to dynamically insert earlier responses or preloaded information like [FIELDNAME] or ${{{{fieldname}}}} is okay as it is.\n",
    "\n",
    "5. Ignore HTML or other formatting, and focus solely on question phrasing (assume that HTML tags will be for visual formatting only and will not be read aloud).\n",
    "\n",
    "When evaluating the question, DON'T:\n",
    "\n",
    "1. Recommend translating something into another language (i.e., suggestions for rephrasing should always be in the same language as the original text).\n",
    "\n",
    "2. Recommend changes in the overall structure of a question (e.g., changing from multiple choice to open-ended or splitting one question into multiple), unless it will substantially improve the quality of the data collected.\n",
    "\n",
    "3. Comment on HTML tags or formatting.\n",
    "\n",
    "Respond in JSON format with all of the following fields:\n",
    "\n",
    "* `Phrases` (list): a list containing all phrases from the excerpt that pre-testing or piloting is likely to identify as problematic (each phrase should be an exact quote)\n",
    "\n",
    "* `Number of phrases` (number): the exact number of phrases in Phrases [ Note that this key must be exactly \"Number of phrases\", with exactly that capitalization and spacing ]\n",
    "\n",
    "* `Recommendations` (list): a list containing suggested replacement phrases, one for each of the phrases in Phrases (in the same order as Phrases; each replacement phrase should be an exact quote that can exactly replace the corresponding phrase in Phrases; and each replacement phrase should be in the same language as the original phrase)\n",
    "\n",
    "* `Explanations` (list): a list containing explanations for why the authors should consider revising each phrase, one for each of the phrases in Phrases (in the same order as Phrases). Do not repeat the entire phrase in the explanation, but feel free to reference specific words or parts as needed.\n",
    "\n",
    "* `Severities` (list): a list containing the severity of each identified issue, one for each of the phrases in Phrases (in the same order as Phrases); each severity should be expressed as a number on a scale from 1 for the least severe issues (minor phrasing issues that are very unlikely to substantively affect responses) to 5 for the most severe issues (problems that are likely to substantively affect responses in a way that introduces bias and/or variance)\n",
    "\n",
    "Raw text:\n",
    "|!|\n",
    "{survey_text}\n",
    "|!|\n",
    "\n",
    "Question to evaluate:\n",
    "|@|\n",
    "{question_text}\n",
    "|@|\n",
    "\n",
    "Your JSON response following the format described above:\"\"\"\n",
    "\n",
    "    # call out to the LLM\n",
    "    print()\n",
    "    print(f\"Evaluating question for phrasing: {question['question']}\")\n",
    "    response_text, response_dict = llm.process_json_response(llm.llm_json_response_with_timeout(phrasing_prompt))\n",
    "\n",
    "    # save and output results\n",
    "    if response_dict is not None:\n",
    "        if 'Number of phrases' in response_dict and response_dict['Number of phrases'] > 0:\n",
    "            print(f\"  Identified {response_dict['Number of phrases']} issue(s)\")\n",
    "            all_results += [response_dict]\n",
    "        else:\n",
    "            print(\"  No issues identified\")\n",
    "    else:\n",
    "        print(f\"  Failed to get a valid response. Response text: {response_text}\")\n",
    "\n",
    "    # set up our bias-review prompt for the LLM\n",
    "    # Note that this prompt was inspired by the example in this blog post:\n",
    "    # https://www.linkedin.com/pulse/using-chatgpt-counter-bias-prejudice-discrimination-johannes-schunter/\n",
    "    bias_prompt = f\"\"\"You are an AI designed to evaluate questionnaires and other survey instruments used by researchers and M&E professionals. You are an expert in survey methodology with training equivalent to a member of the American Association for Public Opinion Research (AAPOR) with a Ph.D. in survey methodology from University of Michiganâ€™s Institute for Social Research. You are also an expert in the areas of gender equality, discrimination, anti-racism, and anti-colonialism. You consider primarily the content, context, and questions provided to you, and then content and methods from the most widely-cited academic publications and public and nonprofit research organizations.\n",
    "\n",
    "You always give truthful, factual answers. When asked to give your response in a specific format, you always give your answer in the exact format requested. You never give offensive responses. If you donâ€™t know the answer to a question, you truthfully say you donâ€™t know.\n",
    "\n",
    "You will be given the raw text from a questionnaire or survey instrument between |!| and |!| delimiters. You will also be given a specific question from that text to evaluate between |@| and |@| delimiters. The question will be supplied in the following format:\n",
    "\n",
    "* Question ID: ID (if any)\n",
    "* Instructions: Instructions (if any)\n",
    "* Question: Question text\n",
    "* Options: Multiple-choice options (if any), with each separated by three pipe symbols (|||) and option values (if any) separated from option labels by three hash symbols (###)\n",
    "\n",
    "Evaluate the question only, but also consider its context within the larger survey.\n",
    "\n",
    "Assume that this survey will be administered by a trained enumerator who asks each question and reads each prompt or instruction as indicated in the excerpt. Your job is to review the question for:\n",
    "\n",
    "a. Stereotypical representations of gender, ethnicity, origin, religion, or other social categories.\n",
    "\n",
    "b. Distorted or biased representations of events, topics, groups, or individuals.\n",
    "\n",
    "c. Use of discriminatory or insensitive language towards certain groups or topics.\n",
    "\n",
    "d. Implicit or explicit assumptions made in the text or unquestioningly adopted that could be based on prejudices.\n",
    "\n",
    "e. Prejudiced descriptions or evaluations of abilities, characteristics, or behaviors.\n",
    "\n",
    "Respond in JSON format with all of the following fields:\n",
    "\n",
    "* `Phrases`: a list containing all problematic phrases from the excerpt that you found in your review (each phrase should be an exact quote from the excerpt)\n",
    "\n",
    "* `Number of phrases`: the exact number of phrases in Phrases [ Note that this key must be exactly \"Number of phrases\", with exactly that capitalization and spacing ]\n",
    "\n",
    "* `Recommendations`: a list containing suggested replacement phrases, one for each of the phrases in Phrases (in the same order as Phrases; each replacement phrase should be an exact quote that can exactly replace the corresponding phrase in Phrases)\n",
    "\n",
    "* `Explanations`: a list containing explanations for why the phrases are problematic, one for each of the phrases in Phrases (in the same order as Phrases)\n",
    "\n",
    "* `Severities`: a list containing the severity of each identified issue, one for each of the phrases in Phrases (in the same order as Phrases); each severity should be expressed as a number on a scale from 1 for the least severe issues (minor phrasing issues that are very unlikely to offend respondents or substantively affect their responses) to 5 for the most severe issues (problems that are very likely to offend respondents or substantively affect responses in a way that introduces bias and/or variance)\n",
    "\n",
    "Raw text:\n",
    "|!|\n",
    "{survey_text}\n",
    "|!|\n",
    "\n",
    "Question to evaluate:\n",
    "|@|\n",
    "{question_text}\n",
    "|@|\n",
    "\n",
    "Your JSON response following the format described above:\"\"\"\n",
    "\n",
    "    # call out to the LLM\n",
    "    print()\n",
    "    print(f\"Evaluating question for bias: {question['question']}\")\n",
    "    response_text, response_dict = llm.process_json_response(llm.llm_json_response_with_timeout(bias_prompt))\n",
    "\n",
    "    # save and output results\n",
    "    if response_dict is not None:\n",
    "        if 'Number of phrases' in response_dict and response_dict['Number of phrases'] > 0:\n",
    "            print(f\"  Identified {response_dict['Number of phrases']} issue(s)\")\n",
    "            all_results += [response_dict]\n",
    "        else:\n",
    "            print(\"  No issues identified\")\n",
    "    else:\n",
    "        print(f\"  Failed to get a valid response. Response text: {response_text}\")"
   ],
   "metadata": {
    "collapsed": true,
    "id": "XfEULWNH_2_i"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Organizing and outputting the results\n",
    "\n",
    "This final code block organizes and outputs final results, saving them in a local file named `survey-review-results.txt`. View or download this file by clicking the file-folder icon in Google Colab's left sidebar."
   ],
   "metadata": {
    "id": "trJ9EDFCWQrR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# generate report\n",
    "if len(all_results) == 0:\n",
    "    report = \"No results to save\"\n",
    "else:\n",
    "    report = \"Survey review results:\\n\"\n",
    "    for result in all_results:\n",
    "        if 'Phrases' in result and result['Number of phrases'] > 0:\n",
    "            # loop through all recommendations, treating lists as parallel arrays\n",
    "            for phrase, recommendation, explanation, severity in zip(result['Phrases'], result['Recommendations'], result['Explanations'], result['Severities']):\n",
    "                report += f\"\\n---\\n\\nSuggest replacing this: {phrase}\\n\\nWith this: {recommendation}\\n\\n{explanation}\\n\\nImportance: {severity} out of 5\\n\"\n",
    "\n",
    "# save the report to file\n",
    "with open(\"survey-review-results.txt\", \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"All recommendations saved to survey-review-results.txt\")"
   ],
   "metadata": {
    "id": "wqn1yzYsWfq5"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
