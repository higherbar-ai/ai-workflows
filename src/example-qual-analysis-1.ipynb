{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyPxP3kjKB/kQGaabIV5w0tX",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/higherbar-ai/ai-workflows/blob/main/src/example-qual-analysis.ipynb\" target=\"_parent\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>\n",
    "\n",
    "# About this example-qual-analysis-1 notebook\n",
    "\n",
    "This notebook provides a simple example of an automated AI workflow, designed to run in [Google Colab](https://colab.research.google.com) or a local environment. It uses [the ai-workflows package](https://github.com/higherbar-ai/ai-workflows) to summarize and perform some basic qualitative analysis on a set of documents. The documents might contain interview transcripts, customer service tickets, or any other text-based data. The notebook will:\n",
    "\n",
    "1. Prompt you to upload or select a `.zip` file with the documents to summarize and analyze\n",
    "\n",
    "2. Extract the text from each document\n",
    "\n",
    "3. Summarize each document\n",
    "\n",
    "4. Identify the top 10 themes present in the documents\n",
    "\n",
    "5. Code each document with the themes identified\n",
    "\n",
    "6. Output the results in a series of `.csv` files\n",
    "\n",
    "See [the ai-workflows GitHub repo](https://github.com/higherbar-ai/ai-workflows) for more details on the `ai_workflows` package.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "This notebook requires different settings depending on which AI service providers you want to use. If you're running in Google Colab, you configure these settings as \"secrets\"; just click the key icon in the left sidebar (and, once you create a secret, be sure to click the toggle to give the notebook access to the secret). If you're running this notebook in a different environment, you can set these settings in a `.env` file; the first time you run, it will write out a template `.env` file for you to fill in and direct you to its location.\n",
    "\n",
    "Following are the settings, regardless of the environment.\n",
    "\n",
    "### OpenAI (direct)\n",
    "\n",
    "To use OpenAI directly:\n",
    "\n",
    "* `openai_api_key` - your OpenAI API key (get one from [the OpenAI API key page](https://platform.openai.com/api-keys), and be sure to fund your platform account with at least $5 to allow GPT-4o model access)\n",
    "* `openai_model` (optional) - the model to use (defaults to `gpt-4o`)\n",
    "\n",
    "### OpenAI (via Microsoft Azure)\n",
    "\n",
    "To use OpenAI via Microsoft Azure:\n",
    "\n",
    "* `azure_api_key` - your Azure API key\n",
    "* `azure_api_base` - the base URL for the Azure API\n",
    "* `azure_api_engine` - the engine to use (a.k.a. the \"deployment\")\n",
    "* `azure_api_version` - the API version to use\n",
    "\n",
    "### Anthropic (direct)\n",
    "\n",
    "To use Anthropic directly:\n",
    "\n",
    "* `anthropic_api_key` - your Anthropic API key\n",
    "* `anthropic_model` - the model to use\n",
    "\n",
    "### LangSmith (for tracing)\n",
    "\n",
    "Optionally, you can add [LangSmith tracing](https://langchain.com/langsmith):\n",
    "\n",
    "* `langsmith_api_key` - your LangSmith API key\n",
    "\n",
    "## Your documents\n",
    "\n",
    "The notebook will prompt you to select or upload a `.zip` file. It should contain all the documents you would like to summarize and analyze.\n",
    "\n",
    "## Setting up the runtime environment\n",
    "\n",
    "This next code block installs all necessary Python and system packages into the current environment.\n",
    "\n",
    "**If you're running in Google Colab and it prompts you to restart the notebook in the middle of the installation steps, just click CANCEL.**"
   ],
   "metadata": {
    "id": "IazipVYi_yUy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# install Google Colab support and ai_workflows package\n",
    "%pip install colab-or-not py-ai-workflows[docs]\n",
    "\n",
    "# download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', force=True)\n",
    "\n",
    "# set up our notebook environment (including LibreOffice)\n",
    "from colab_or_not import NotebookBridge\n",
    "notebook_env = NotebookBridge(\n",
    "    system_packages=[\"libreoffice\"],\n",
    "    config_path=\"~/.hbai/ai-workflows.env\",\n",
    "    config_template={\n",
    "        \"openai_api_key\": \"\",\n",
    "        \"openai_model\": \"\",\n",
    "        \"azure_api_key\": \"\",\n",
    "        \"azure_api_base\": \"\",\n",
    "        \"azure_api_engine\": \"\",\n",
    "        \"azure_api_version\": \"\",\n",
    "        \"anthropic_api_key\": \"\",\n",
    "        \"anthropic_model\": \"\",\n",
    "        \"langsmith_api_key\": \"\",\n",
    "    }\n",
    ")\n",
    "notebook_env.setup_environment()"
   ],
   "metadata": {
    "collapsed": true,
    "id": "UGxo3IreHmUZ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initializing for AI workflows\n",
    "\n",
    "The next code block initializes the notebook by loading settings and initializing the LLM interface."
   ],
   "metadata": {
    "id": "SGJyVWA1Ua-v"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j9coSno4_Arf"
   },
   "source": [
    "from ai_workflows.llm_utilities import LLMInterface\n",
    "from ai_workflows.document_utilities import DocumentInterface\n",
    "\n",
    "# read all supported secrets\n",
    "openai_api_key = notebook_env.get_setting('openai_api_key')\n",
    "openai_model = notebook_env.get_setting('openai_model', 'gpt-4o')\n",
    "azure_api_key = notebook_env.get_setting('azure_api_key')\n",
    "azure_api_base = notebook_env.get_setting('azure_api_base')\n",
    "azure_api_engine = notebook_env.get_setting('azure_api_engine')\n",
    "azure_api_version = notebook_env.get_setting('azure_api_version')\n",
    "anthropic_api_key = notebook_env.get_setting(\"anthropic_api_key\")\n",
    "anthropic_model = notebook_env.get_setting(\"anthropic_model\")\n",
    "langsmith_api_key = notebook_env.get_setting('langsmith_api_key')\n",
    "\n",
    "# complain if we don't have the bare minimum to run\n",
    "if (not openai_api_key\n",
    "        and not (azure_api_key and azure_api_base and azure_api_engine and azure_api_version)\n",
    "        and not (anthropic_api_key and anthropic_model)):\n",
    "    raise Exception('We need settings set for OpenAI access (direct or via Azure) or for Anthropic access (direct). See the instructions above for more details.')\n",
    "\n",
    "# initialize LLM interface\n",
    "llm = LLMInterface(openai_api_key=openai_api_key, openai_model=openai_model, azure_api_key=azure_api_key, azure_api_base=azure_api_base, azure_api_engine=azure_api_engine, azure_api_version=azure_api_version, temperature = 0.0, total_response_timeout_seconds=600, number_of_retries=2, seconds_between_retries=5, langsmith_api_key=langsmith_api_key, anthropic_api_key=anthropic_api_key, anthropic_model=anthropic_model)\n",
    "\n",
    "# initialize two document processors, one with an LLM and one without\n",
    "doc_processor = DocumentInterface()\n",
    "doc_processor_llm = DocumentInterface(llm_interface=llm)\n",
    "\n",
    "# set max tokens to consider from each document (120,000 tokens is about 90,000 words or 180 pages)\n",
    "max_doc_tokens = 120000\n",
    "\n",
    "# report success\n",
    "print(\"Initialization successful.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prompting for your documents\n",
    "\n",
    "This next code block prompts you to upload or select a `.zip` file with the documents to summarize and analyze.\n",
    "\n",
    "If you don't have any documents handy, you can use [this set of example interview transcripts](https://github.com/higherbar-ai/ai-workflows/blob/main/resources/sample_orda_interviews.zip). These come from the *Fostering cultures of open qualitative research* project and were originally retrieved [from the ORDA repository here](https://orda.shef.ac.uk/articles/dataset/Fostering_cultures_of_open_qualitative_research_Dataset_2_Interview_Transcripts/23567223)."
   ],
   "metadata": {
    "id": "F8gvZYWeSpPD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# prompt for a single .zip file and keep prompting till we get one\n",
    "file_path = \"\"\n",
    "while True:\n",
    "    # prompt for a .zip file\n",
    "    selected_files = notebook_env.get_input_files(\".zip file with documents:\")\n",
    "\n",
    "    # complain if we didn't get just a single file\n",
    "    if len(selected_files) != 1 or not selected_files[0].endswith('.zip'):\n",
    "        print()\n",
    "        print('Please upload a single .zip file to continue.')\n",
    "        print()\n",
    "    else:\n",
    "        # fetch the path of the uploaded file\n",
    "        file_path = selected_files[0]\n",
    "        \n",
    "        # break from loop\n",
    "        break\n",
    "\n",
    "# report results\n",
    "print()\n",
    "print(f\"Will process this .zip file: {file_path}\")"
   ],
   "metadata": {
    "collapsed": true,
    "id": "u0BAoiTHIJnd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summarizing the documents\n",
    "\n",
    "The next code block runs through each document, using the LLM to summarize it. Feel free to adjust the instructions used to guide the summarization to meet your needs.\n",
    "\n",
    "What we summarize here is the raw text extracted from each document. If your documents include figures, images, or complex layouts, you may want to use an LLM to read the document in a higher-quality (but slower and more-expensive) manner. You can do this by simply changing `doc_processor.convert_to_markdown(unzipped_file_path)` to `doc_processor_llm.convert_to_markdown(unzipped_file_path)` in the code block below.\n"
   ],
   "metadata": {
    "id": "elA3S_P-9_MU"
   }
  },
  {
   "metadata": {
    "id": "DESt6PAj-WKh"
   },
   "cell_type": "code",
   "source": [
    "import tempfile\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# create a list to store document details\n",
    "documents = []\n",
    "\n",
    "# create a temporary directory\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # unzip the .zip file into the temporary directory\n",
    "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(temp_dir)\n",
    "\n",
    "    # loop through each file in the temporary directory\n",
    "    for root, dirs, files in os.walk(temp_dir):\n",
    "        for unzipped_file in files:\n",
    "            unzipped_file_path = os.path.join(root, unzipped_file)\n",
    "            if unzipped_file.startswith('.'):\n",
    "                # skip hidden files\n",
    "                continue\n",
    "\n",
    "            print()\n",
    "            print(f\"Reading {unzipped_file}...\")\n",
    "            # if you want to use an LLM to read the file in a higher-quality (but slower and more-expensive) manner, you can use the following line instead:\n",
    "            # doc_text = doc_processor_llm.convert_to_markdown(unzipped_file_path)\n",
    "            doc_text = doc_processor.convert_to_markdown(unzipped_file_path)\n",
    "\n",
    "            print(f\"Summarizing {unzipped_file}...\")\n",
    "\n",
    "            # provide context so that the LLM knows what it's looking at\n",
    "            json_context = \"The file contains a document being processed for qualitative analysis.\"\n",
    "\n",
    "            # provide a summary of the job to be done (in this case, extracting a title and summarizing the document for qualitative analysis)\n",
    "            json_job = f\"\"\"Your job is to:\n",
    "\n",
    "1. Extract or create an appropriate title for the document\n",
    "\n",
    "2. Summarize the document's contents in a short, concise form that is between one and three paragraphs in length. The summarized form should retain those details from the original file necessary to inform the qualitative analysis (subjects discussed, themes, key points, etc.).\n",
    "\n",
    "Only give a truthful and faithful title and summary. Never invent any details or expand upon the file's contents in any way (except in creating a title if the document doesn't begin with an appropriate title).\"\"\"\n",
    "\n",
    "            # provide the exact JSON format expected in the output\n",
    "            json_output_spec = f\"\"\"Return JSON with the following fields (and only the following fields):\n",
    "\n",
    "* `title` (string): A suitable title for the document. If a suitable title appears near the beginning of the document, use that exactly. Otherwise, do your best to create an appropriate title based on the content of the document.\n",
    "\n",
    "* `summary` (string): A short, concise summary of the document's contents, between one and three paragraphs in length. The summary should retain those details from the original file necessary to inform the qualitative analysis (subjects discussed, themes, key points, etc.).\"\"\"\n",
    "\n",
    "            # process the file\n",
    "            all_responses = doc_processor_llm.markdown_to_json(markdown=doc_text, json_context=json_context, json_job=json_job, json_output_spec=json_output_spec, max_chunk_size=max_doc_tokens)\n",
    "            response = all_responses[0]\n",
    "            if len(all_responses) > 1:\n",
    "                # if we had to split the document to summarize it in pieces, we're just going to go with the first piece for simplicity\n",
    "                # (if we wanted, we could use the first title and use the LLM to combine the separate summaries into a single summary)\n",
    "                total_doc_tokens = llm.count_tokens(doc_text)\n",
    "                print(f\"  Warning: only summarized first {max_doc_tokens} of {total_doc_tokens} tokens in document\")\n",
    "\n",
    "            # save results\n",
    "            documents.append({\n",
    "                \"file\": unzipped_file,\n",
    "                \"text\": doc_text,\n",
    "                \"title\": response['title'],\n",
    "                \"summary\": response['summary']\n",
    "            })\n",
    "\n",
    "print()\n",
    "print(f\"Completed summarization of {len(documents)} documents.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Identifying the top 10 subjects or themes\n",
    "\n",
    "This next code block will use the document titles and summaries to identify the top 10 subjects or themes present in the documents. Feel free to adjust the instructions used to guide the theme identification to meet your needs.\n",
    "\n",
    "If the total number of tokens in the titles and summaries exceeds our limit (120,000 tokens, which is about 90,000 words or 180 pages), we'll truncate the text to fit within the limit. We could also use the LLM to combine the results from multiple runs if we wanted to analyze the full text, but likely the first 180 pages of document summaries is far more than enough to identify the top themes."
   ],
   "metadata": {
    "id": "CQLAfiV8MlXM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# assemble all titles and summaries into a single text block\n",
    "all_summaries = \"\"\n",
    "for document in documents:\n",
    "    all_summaries += f\"**{document['title']}**\\n{document['summary']}\\n\\n\"\n",
    "\n",
    "# truncate the summaries if needed (to avoid overflowing LLM context window)\n",
    "summaries_tokens = llm.count_tokens(all_summaries)\n",
    "if summaries_tokens > max_doc_tokens:\n",
    "    all_summaries = llm.enforce_max_tokens(all_summaries, max_doc_tokens)\n",
    "    print()\n",
    "    print(f\"  Warning: only considering first {max_doc_tokens} of {summaries_tokens} tokens in list of document titles and summaries\")\n",
    "    print()\n",
    "\n",
    "print()\n",
    "print(f\"Identifying themes...\")\n",
    "\n",
    "# describe the job we want the LLM to do\n",
    "job_description = \"\"\"I'm performing a qualitative analysis on a set of documents. All of the document titles and summaries are included below. Your job is to identify the top 10 themes or subjects discussed in the documents, returning them in the exact JSON format described below. The top 10 you choose should be based on the frequency with which they appear in the documents summarized below. Perform the analysis as if you were an expert in qualitative analysis trained in a top graduate program.\"\"\"\n",
    "\n",
    "# describe the exact JSON format we expect back from the LLM\n",
    "json_output_spec = \"\"\"Return JSON with the following fields (and only the following fields):\n",
    "\n",
    "* `themes` (list of objects): The list of top themes or subjects discussed in the documents, each of which should be an object with the following fields:\n",
    "\n",
    "    * `id` (string): A short, concise identifier for the theme or subject. This should be alphanumeric and contain no spaces or special characters.\n",
    "\n",
    "    * `description` (string): A short description of the theme or subject. This can be as short as a single sentence fragment or as long as a short paragraph, depending on the complexity of the subject or theme. The description should be sufficient for a human qualitative coder to be able to code individual documents.\"\"\"\n",
    "\n",
    "# assemble the overall prompt\n",
    "json_prompt = f\"\"\"{job_description}\n",
    "\n",
    "{json_output_spec}\n",
    "\n",
    "All document titles and summaries enclosed by |@| delimiters:\n",
    "\n",
    "|@|{all_summaries}|@|\n",
    "\n",
    "Your JSON response precisely following the instructions given above the titles and summaries:\"\"\"\n",
    "\n",
    "# execute the LLM query, with automatic JSON validation+retry\n",
    "parsed_response, raw_response, error = llm.get_json_response(prompt=json_prompt, json_validation_desc=json_output_spec)\n",
    "\n",
    "# save and report results\n",
    "if error:\n",
    "    print()\n",
    "    print(f\"Error: {error}\")\n",
    "\n",
    "    themes = []\n",
    "else:\n",
    "    themes = parsed_response['themes']\n",
    "    print()\n",
    "    print(f\"Identified {len(themes)} themes:\")\n",
    "    for theme in themes:\n",
    "        print()\n",
    "        print(f\"  {theme['id']}: {theme['description']}\")"
   ],
   "metadata": {
    "collapsed": true,
    "id": "XfEULWNH_2_i"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Coding each document with the identified themes, extracting example quotes\n",
    "\n",
    "This next code block will code each document with the themes identified and extract one example quote from each document. Feel free to adjust the instructions used to guide the coding and quote extraction to meet your needs.\n",
    "\n",
    "If we wanted this to be faster and cheaper, we could code documents based on their summaries rather than going back to their full text. However, going back to the full text allows us to be more thorough â€” and it allows us to extract example quotes from the original text."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# if no themes identified, raise an error\n",
    "if not themes:\n",
    "    raise Exception(\"No themes identified, so we can't code the documents\")\n",
    "\n",
    "# loop through each document\n",
    "error_count = 0\n",
    "last_error = \"\"\n",
    "for document in documents:\n",
    "    print()\n",
    "    print(f\"Coding {document['file']}...\")\n",
    "\n",
    "    # get document text and truncate if needed\n",
    "    doc_text = document['text']\n",
    "    doc_tokens = llm.count_tokens(doc_text)\n",
    "    if doc_tokens > max_doc_tokens:\n",
    "        doc_text = llm.enforce_max_tokens(doc_text, max_doc_tokens)\n",
    "        print(f\"  Warning: only considering first {max_doc_tokens} of {doc_tokens} tokens in document\")\n",
    "\n",
    "    # describe the job we want the LLM to do\n",
    "    job_description = f\"\"\"I'm performing a qualitative analysis on a set of documents. I'm going to give you the text from one of the documents, and I need you to do two things:\n",
    "\n",
    "1. Identify which of the themes or subjects (if any) is present in the document, based on the JSON list I provide. Be sure to include all themes or subjects that are present in the document, even if they are only mentioned once.\n",
    "\n",
    "2. If you identified at least one theme or subject present in the document, provide an exact quote from the document that exemplifies or supports one of the themes or subjects that you identified. This quote should be suitable for use in the final analysis, as an example of how the theme or subject appears in the document's text.\n",
    "\n",
    "Perform the analysis as if you were an expert in qualitative analysis trained in a top graduate program.\n",
    "\n",
    "This is the list of themes or subjects in JSON format:\n",
    "\n",
    "```\n",
    "{themes}\n",
    "```\"\"\"\n",
    "\n",
    "    # describe the exact JSON format we expect back from the LLM\n",
    "    json_output_spec = \"\"\"Return JSON with the following fields (and only the following fields):\n",
    "\n",
    "* `themes` (list of objects): The list of ALL themes or subjects discussed in the document (be sure not to miss any), each of which should be an object with the following fields:\n",
    "\n",
    "    * `id` (string): The short identifier for the theme or subject (from the list provided).\n",
    "\n",
    "* `exact_quote` (string): An exact quote from the document that exemplifies or supports one of the themes or subjects identified in the `themes` list (or \"\" if no themes identified). Try to choose a quote that does a good job of illustrating the theme or subject, if possible.\n",
    "\n",
    "* `exact_quote_theme_id` (string): The short identifier for the theme or subject that the `exact_quote` exemplifies or supports (must match an `id` supplied in the `themes` list, or \"\" if no themes identified).\"\"\"\n",
    "\n",
    "    # assemble the overall prompt\n",
    "    json_prompt = f\"\"\"{job_description}\n",
    "\n",
    "{json_output_spec}\n",
    "\n",
    "The full document text enclosed by |@| delimiters:\n",
    "\n",
    "|@|{doc_text}|@|\n",
    "\n",
    "Your JSON response precisely following the instructions given above the titles and summaries:\"\"\"\n",
    "\n",
    "    # execute the LLM query, with automatic JSON validation+retry\n",
    "    parsed_response, raw_response, error = llm.get_json_response(prompt=json_prompt, json_validation_desc=json_output_spec)\n",
    "\n",
    "    # save and report results\n",
    "    if error:\n",
    "        document['themes'] = \"\"\n",
    "        document['example_quote'] = \"\"\n",
    "        document['example_quote_theme_id'] = \"\"\n",
    "\n",
    "        print(f\"  Error during coding: {error}\")\n",
    "\n",
    "        error_count += 1\n",
    "        last_error = error\n",
    "    else:\n",
    "        document['themes'] = parsed_response['themes']\n",
    "        document['example_quote'] = parsed_response['exact_quote']\n",
    "        document['example_quote_theme_id'] = parsed_response['exact_quote_theme_id']\n",
    "\n",
    "        print(f\"  Coded with themes: {', '.join([theme['id'] for theme in parsed_response['themes']])}\")\n",
    "\n",
    "# report overall results\n",
    "if error_count > 0:\n",
    "    print()\n",
    "    print(f\"Some documents could not be coded due to errors.\")\n",
    "    print(f\"  Total errors encountered: {error_count}\")\n",
    "    print(f\"  Last error encountered: {last_error}\")\n",
    "else:\n",
    "    print()\n",
    "    print(f\"All {len(documents)} documents coded successfully.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Organizing and outputting the results\n",
    "\n",
    "This final code block organizes and outputs final results, saving them in a file named `survey-review-results.txt`.\n",
    "\n",
    "If you're running in Google Colab, this file will be saved into the content folder. Find, view, and download it by clicking on the folder icon in the left sidebar.\n",
    "\n",
    "If you're running elsewhere, it will be saved into an `ai-workflows` subdirectory created off of your user home directory."
   ],
   "metadata": {
    "id": "trJ9EDFCWQrR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# output files to ~/ai-workflows directory if local, otherwise /content if Google Colab\n",
    "output_path_prefix = notebook_env.get_output_dir(not_colab_dir=\"~/ai-workflows\", colab_subdir=\"\")\n",
    "\n",
    "# output theme list to UTF-8 .csv file\n",
    "themes_output_file = os.path.join(output_path_prefix, \"example-qual-analysis-1-themes.csv\")\n",
    "with open(themes_output_file, \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"description\"])\n",
    "    for theme in themes:\n",
    "        writer.writerow([theme['id'], theme['description']])\n",
    "\n",
    "# output document list to UTF-8 .csv file in wide format\n",
    "docs_output_file = os.path.join(output_path_prefix, \"example-qual-analysis-1-documents.csv\")\n",
    "theme_ids = [theme['id'] for theme in themes]\n",
    "with open(docs_output_file, \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write header\n",
    "    writer.writerow([\"file\", \"title\", \"summary\", \"example_quote\", \"example_quote_theme_id\"] + theme_ids)\n",
    "\n",
    "    # write document data\n",
    "    for document in documents:\n",
    "        theme_presence = [1 if theme_id in [t['id'] for t in document['themes']] else 0 for theme_id in theme_ids]\n",
    "        writer.writerow([\n",
    "            document['file'],\n",
    "            document['title'],\n",
    "            document['summary'],\n",
    "            document['example_quote'],\n",
    "            document['example_quote_theme_id']\n",
    "        ] + theme_presence)\n",
    "\n",
    "print()\n",
    "print(f\"Themes saved to {themes_output_file}\")\n",
    "print(f\"Document summaries, codes, and example quotes saved to {docs_output_file}\")"
   ],
   "metadata": {
    "id": "wqn1yzYsWfq5"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
