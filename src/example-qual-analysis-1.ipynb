{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyPxP3kjKB/kQGaabIV5w0tX",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/higherbar-ai/ai-workflows/blob/main/src/example-qual-analysis-1.ipynb\" target=\"_parent\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>\n",
    "\n",
    "# About this example-qual-analysis-1 notebook\n",
    "\n",
    "This notebook provides a simple example of an automated AI workflow, designed to run in [Google Colab](https://colab.research.google.com) or a local environment. It uses [the ai-workflows package](https://github.com/higherbar-ai/ai-workflows) to perform some basic qualitative analysis on a set of interview transcripts. The notebook will:\n",
    "\n",
    "1. Prompt you to upload or select a `.zip` file with one document per interview transcript\n",
    "\n",
    "2. Extract the text from each transcript\n",
    "\n",
    "3. Summarize each transcript\n",
    "\n",
    "4. Develop and refine a codebook for coding the transcripts\n",
    "\n",
    "5. Code each document\n",
    "\n",
    "6. Output the results in a series of `.csv` files\n",
    "\n",
    "You can easily adapt this workflow to other kinds of document-based qualitative analyses. Just adjust the text that guides the LLM in summarizing the documents, developing the codebook, and coding the documents.\n",
    "\n",
    "See [the ai-workflows GitHub repo](https://github.com/higherbar-ai/ai-workflows) for more details on the `ai_workflows` package.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "This notebook requires different settings depending on which AI service providers you want to use. If you're running in Google Colab, you configure these settings as \"secrets\"; just click the key icon in the left sidebar (and, once you create a secret, be sure to click the toggle to give the notebook access to the secret). If you're running this notebook in a different environment, you can set these settings in a `.env` file; the first time you run, it will write out a template `.env` file for you to fill in and direct you to its location.\n",
    "\n",
    "Following are the settings, regardless of the environment.\n",
    "\n",
    "### OpenAI (direct)\n",
    "\n",
    "To use OpenAI directly:\n",
    "\n",
    "* `openai_api_key` - your OpenAI API key (get one from [the OpenAI API key page](https://platform.openai.com/api-keys), and be sure to fund your platform account with at least $5 to allow GPT-4o model access)\n",
    "* `openai_model` (optional) - the model to use (defaults to `gpt-4o`)\n",
    "\n",
    "### OpenAI (via Microsoft Azure)\n",
    "\n",
    "To use OpenAI via Microsoft Azure:\n",
    "\n",
    "* `azure_api_key` - your Azure API key\n",
    "* `azure_api_base` - the base URL for the Azure API\n",
    "* `azure_api_engine` - the engine to use (a.k.a. the \"deployment\")\n",
    "* `azure_api_version` - the API version to use\n",
    "\n",
    "### Anthropic (direct)\n",
    "\n",
    "To use Anthropic directly:\n",
    "\n",
    "* `anthropic_api_key` - your Anthropic API key\n",
    "* `anthropic_model` - the model to use\n",
    "\n",
    "### LangSmith (for tracing)\n",
    "\n",
    "Optionally, you can add [LangSmith tracing](https://langchain.com/langsmith):\n",
    "\n",
    "* `langsmith_api_key` - your LangSmith API key\n",
    "\n",
    "## Your documents\n",
    "\n",
    "The notebook will prompt you to select or upload a `.zip` file. It should contain all the transcripts you would like to summarize and analyze.\n",
    "\n",
    "## Setting up the runtime environment\n",
    "\n",
    "This next code block installs all necessary Python and system packages into the current environment.\n",
    "\n",
    "**If you're running in Google Colab and it prompts you to restart the notebook in the middle of the installation steps, just click CANCEL.**"
   ],
   "metadata": {
    "id": "IazipVYi_yUy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# install Google Colab support and ai_workflows package\n",
    "%pip install colab-or-not py-ai-workflows[docs]\n",
    "\n",
    "# download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', force=True)\n",
    "\n",
    "# set up our notebook environment (including LibreOffice)\n",
    "from colab_or_not import NotebookBridge\n",
    "notebook_env = NotebookBridge(\n",
    "    system_packages=[\"libreoffice\"],\n",
    "    config_path=\"~/.hbai/ai-workflows.env\",\n",
    "    config_template={\n",
    "        \"openai_api_key\": \"\",\n",
    "        \"openai_model\": \"\",\n",
    "        \"azure_api_key\": \"\",\n",
    "        \"azure_api_base\": \"\",\n",
    "        \"azure_api_engine\": \"\",\n",
    "        \"azure_api_version\": \"\",\n",
    "        \"anthropic_api_key\": \"\",\n",
    "        \"anthropic_model\": \"\",\n",
    "        \"langsmith_api_key\": \"\",\n",
    "    }\n",
    ")\n",
    "notebook_env.setup_environment()"
   ],
   "metadata": {
    "collapsed": true,
    "id": "UGxo3IreHmUZ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initializing for AI workflows\n",
    "\n",
    "The next code block initializes the notebook by loading settings and initializing the LLM interface."
   ],
   "metadata": {
    "id": "SGJyVWA1Ua-v"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j9coSno4_Arf"
   },
   "source": [
    "from ai_workflows.llm_utilities import LLMInterface\n",
    "from ai_workflows.document_utilities import DocumentInterface\n",
    "\n",
    "# read all supported secrets\n",
    "openai_api_key = notebook_env.get_setting('openai_api_key')\n",
    "openai_model = notebook_env.get_setting('openai_model', 'gpt-4o')\n",
    "azure_api_key = notebook_env.get_setting('azure_api_key')\n",
    "azure_api_base = notebook_env.get_setting('azure_api_base')\n",
    "azure_api_engine = notebook_env.get_setting('azure_api_engine')\n",
    "azure_api_version = notebook_env.get_setting('azure_api_version')\n",
    "anthropic_api_key = notebook_env.get_setting(\"anthropic_api_key\")\n",
    "anthropic_model = notebook_env.get_setting(\"anthropic_model\")\n",
    "langsmith_api_key = notebook_env.get_setting('langsmith_api_key')\n",
    "\n",
    "# complain if we don't have the bare minimum to run\n",
    "if (not openai_api_key\n",
    "        and not (azure_api_key and azure_api_base and azure_api_engine and azure_api_version)\n",
    "        and not (anthropic_api_key and anthropic_model)):\n",
    "    raise Exception('We need settings set for OpenAI access (direct or via Azure) or for Anthropic access (direct). See the instructions above for more details.')\n",
    "\n",
    "# initialize LLM interface\n",
    "llm = LLMInterface(openai_api_key=openai_api_key, openai_model=openai_model, azure_api_key=azure_api_key, azure_api_base=azure_api_base, azure_api_engine=azure_api_engine, azure_api_version=azure_api_version, temperature = 0.0, total_response_timeout_seconds=600, number_of_retries=2, seconds_between_retries=5, langsmith_api_key=langsmith_api_key, anthropic_api_key=anthropic_api_key, anthropic_model=anthropic_model)\n",
    "\n",
    "# initialize two document processors, one with an LLM and one without\n",
    "doc_processor = DocumentInterface()\n",
    "doc_processor_llm = DocumentInterface(llm_interface=llm)\n",
    "\n",
    "# set max tokens to consider from each document (120,000 tokens is about 90,000 words or 180 pages)\n",
    "max_doc_tokens = 120000\n",
    "\n",
    "# report success\n",
    "print(\"Initialization successful.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prompting for your transcripts\n",
    "\n",
    "This next code block prompts you to upload or select a `.zip` file with the transcripts to summarize and analyze (assumes one transcript per document).\n",
    "\n",
    "If you don't have any transcripts handy, you can use [this set of example interview transcripts](https://github.com/higherbar-ai/ai-workflows/blob/main/resources/sample_orda_interviews.zip). These come from the *Fostering cultures of open qualitative research* project and were originally retrieved [from the ORDA repository here](https://orda.shef.ac.uk/articles/dataset/Fostering_cultures_of_open_qualitative_research_Dataset_2_Interview_Transcripts/23567223)."
   ],
   "metadata": {
    "id": "F8gvZYWeSpPD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# prompt for a single .zip file and keep prompting till we get one\n",
    "file_path = \"\"\n",
    "while True:\n",
    "    # prompt for a .zip file\n",
    "    selected_files = notebook_env.get_input_files(\".zip file with transcripts:\")\n",
    "\n",
    "    # complain if we didn't get just a single file\n",
    "    if len(selected_files) != 1 or not selected_files[0].endswith('.zip'):\n",
    "        print()\n",
    "        print('Please upload a single .zip file with your transcripts to continue.')\n",
    "        print()\n",
    "    else:\n",
    "        # fetch the path of the uploaded file\n",
    "        file_path = selected_files[0]\n",
    "        \n",
    "        # break from loop\n",
    "        break\n",
    "\n",
    "# report results\n",
    "print()\n",
    "print(f\"Will process transcripts in this .zip file: {file_path}\")"
   ],
   "metadata": {
    "collapsed": true,
    "id": "u0BAoiTHIJnd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summarizing the transcripts\n",
    "\n",
    "The next code block runs through each transcript, using the LLM to summarize it. Feel free to adjust the instructions used to guide the summarization to meet your needs.\n",
    "\n",
    "*Note:* What we summarize here is the raw text extracted from each document. If your documents include figures, images, or complex layouts, you may want to use an LLM to read the document in a higher-quality (but slower and more-expensive) manner. You can do this by simply changing `doc_processor.convert_to_markdown(unzipped_file_path)` to `doc_processor_llm.convert_to_markdown(unzipped_file_path)` in the code block below.\n"
   ],
   "metadata": {
    "id": "elA3S_P-9_MU"
   }
  },
  {
   "metadata": {
    "id": "DESt6PAj-WKh"
   },
   "cell_type": "code",
   "source": [
    "import tempfile\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# create a list to store document details\n",
    "documents = []\n",
    "\n",
    "# create a temporary directory\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # unzip the .zip file into the temporary directory\n",
    "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(temp_dir)\n",
    "\n",
    "    # loop through each file in the temporary directory\n",
    "    for root, dirs, files in os.walk(temp_dir):\n",
    "        for unzipped_file in files:\n",
    "            unzipped_file_path = os.path.join(root, unzipped_file)\n",
    "            if unzipped_file.startswith('.'):\n",
    "                # skip hidden files\n",
    "                continue\n",
    "\n",
    "            print()\n",
    "            print(f\"Reading {unzipped_file}...\")\n",
    "            # if you want to use an LLM to read the file in a higher-quality (but slower and more-expensive) manner, you can use the following line instead:\n",
    "            # doc_text = doc_processor_llm.convert_to_markdown(unzipped_file_path)\n",
    "            doc_text = doc_processor.convert_to_markdown(unzipped_file_path)\n",
    "\n",
    "            print(f\"Summarizing {unzipped_file}...\")\n",
    "\n",
    "            # provide context so that the LLM knows what it's looking at\n",
    "            json_context = \"The file contains a single interview transcript.\"\n",
    "\n",
    "            # provide a summary of the job to be done\n",
    "            json_job = f\"\"\"Your job is to read the transcript carefully and then produce a concise summary that highlights the main topics, recurring ideas, significant opinions, and emotional tones expressed by the participant(s). Please include key details such as the participant’s role or background if evident, the issues or themes they discuss, any problems or challenges mentioned, any proposed solutions or actions, and any notable conflicts or contradictions. If the participant shows particular feelings (e.g., frustration, excitement, uncertainty), describe those. If the participant refers to external events, organizational structures, or specific stakeholders, include that information. The goal is for the summary to capture enough richness and detail so that a subsequent analysis could generate meaningful qualitative codes from it.\"\"\"\n",
    "\n",
    "            # provide the exact JSON format expected in the output\n",
    "            json_output_spec = f\"\"\"Return JSON with the following fields (and only the following fields):\n",
    "\n",
    "* `summary` (string): Your summary of the transcript, following the instructions above.\"\"\"\n",
    "\n",
    "            # process the file\n",
    "            all_responses = doc_processor_llm.markdown_to_json(markdown=doc_text, json_context=json_context, json_job=json_job, json_output_spec=json_output_spec, max_chunk_size=max_doc_tokens)\n",
    "            response = all_responses[0]\n",
    "            if len(all_responses) > 1:\n",
    "                # if we had to split the document to summarize it in pieces, we're just going to go with the first piece for simplicity\n",
    "                # (if we wanted, we could use the first title and use the LLM to combine the separate summaries into a single summary)\n",
    "                total_doc_tokens = llm.count_tokens(doc_text)\n",
    "                print(f\"  Warning: only summarized first {max_doc_tokens} of {total_doc_tokens} tokens in document\")\n",
    "\n",
    "            # save results\n",
    "            documents.append({\n",
    "                \"file\": unzipped_file,\n",
    "                \"text\": doc_text,\n",
    "                \"summary\": response['summary']\n",
    "            })\n",
    "\n",
    "print()\n",
    "print(f\"Completed summarization of {len(documents)} transcripts.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Developing the codebook\n",
    "\n",
    "This next code block will use the document summaries to develop a codebook for coding the transcripts, then it will refine that codebook. Feel free to adjust the instructions used to guide the codebook generation to meet your needs.\n",
    "\n",
    "For each codebook draft, we randomize the order of summaries to mitigate any bias that might arise from the order of the transcripts.\n",
    "\n",
    "If the total number of tokens in the summaries exceeds our limit (120,000 tokens, which is about 90,000 words or 180 pages), we'll truncate the text to fit within the limit. (We could also use the LLM to combine the results from multiple runs if we wanted to include all summaries in the analysis, but likely the first 180 pages of document summaries is more than enough to identify appropriate codes.)\n",
    "\n",
    "### Using your own codebook\n",
    "\n",
    "If you prefer, you can replace this next code block with a much simpler one that just sets up your own codebook for the next step. Here's the format to follow:\n",
    "\n",
    "```\n",
    "codes = [\n",
    "    {\n",
    "        \"id\": \"code1\",\n",
    "        \"label\": \"Label for Code 1\",\n",
    "        \"definition\": \"Definition for Code 1\",\n",
    "        \"example\": \"Example for Code 1\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"code2\",\n",
    "        \"label\": \"Label for Code 2\",\n",
    "        \"definition\": \"Definition for Code 2\",\n",
    "        \"example\": \"Example for Code 2\"\n",
    "    }\n",
    "]\n",
    "```"
   ],
   "metadata": {
    "id": "CQLAfiV8MlXM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import random, json\n",
    "\n",
    "# assemble all titles and summaries into a single text block (in random order)\n",
    "def get_all_summaries(docs) -> str:\n",
    "    # randomize the order of the documents\n",
    "    random.seed()\n",
    "    docs = random.sample(docs, len(docs))\n",
    "\n",
    "    # assemble all the summaries\n",
    "    retval = \"\"\n",
    "    doc_number = 0\n",
    "    for doc in docs:\n",
    "        doc_number += 1\n",
    "        retval += f\"### Transcript {doc_number}\\n{doc['summary']}\\n\\n\"\n",
    "\n",
    "    # truncate the summaries if needed (to avoid overflowing LLM context window)\n",
    "    summaries_tokens = llm.count_tokens(retval)\n",
    "    if summaries_tokens > max_doc_tokens:\n",
    "        retval = llm.enforce_max_tokens(retval, max_doc_tokens)\n",
    "        print()\n",
    "        print(f\"  Warning: only considering first {max_doc_tokens} of {summaries_tokens} tokens in list of summaries\")\n",
    "        print()\n",
    "\n",
    "    return retval\n",
    "\n",
    "# get a randomized list of transcript summaries\n",
    "all_summaries = get_all_summaries(documents)\n",
    "\n",
    "print()\n",
    "print(f\"Creating first-draft codebook...\")\n",
    "\n",
    "# describe the job we want the LLM to do\n",
    "job_description = \"\"\"Below are summaries of multiple interview transcripts. Each summary encapsulates key topics, opinions, emotions, contexts, and dynamics present in the original data. Please read these summaries carefully and then propose an initial codebook for qualitative analysis.\n",
    "\n",
    "Your codebook should:\n",
    "\n",
    "1. Identify key recurring concepts, patterns, and themes that emerge across the summaries.\n",
    "2. Provide a a short, unique identifier for each code.\n",
    "3. Provide a short, descriptive label for each code.\n",
    "4. Include a concise definition of what the code represents (i.e., when and how it should be applied).\n",
    "5. Offer an illustrative example (excerpt or paraphrase) from the summaries that demonstrates the use of each code.\n",
    "\n",
    "The goal is to produce a practical, clearly defined set of codes that could guide a researcher or analyst in systematically coding the full dataset.\"\"\"\n",
    "\n",
    "# describe the exact JSON format we expect back from the LLM\n",
    "json_output_spec = \"\"\"Return JSON with the following fields (and only the following fields):\n",
    "\n",
    "* `codes` (list of objects): The list of proposed codes, each of which should be an object with the following fields:\n",
    "\n",
    "* `id` (string): A short, concise identifier for the code. This should be alphanumeric and contain no spaces or special characters.\n",
    "\n",
    "* `label` (string): A short, descriptive label for the code. This should be in the form of a descriptive sentence fragment, as might be used in a bulleted list of codes.\n",
    "\n",
    "* `definition` (string): A concise definition of what the code represents (i.e., when and how it should be applied). This can be between one sentence and one paragraph in length.\n",
    "\n",
    "* `example` (string): An illustrative example (excerpt or paraphrase) from the summaries that demonstrates the use of the code. This should help coders to better understand when to use the code.\"\"\"\n",
    "\n",
    "# assemble the overall prompt\n",
    "json_prompt = f\"\"\"{job_description}\n",
    "\n",
    "{json_output_spec}\n",
    "\n",
    "All transcript summaries enclosed in a code block:\n",
    "\n",
    "```\n",
    "{all_summaries}\n",
    "```\n",
    "\n",
    "Your JSON response precisely following the instructions given above the summaries:\"\"\"\n",
    "\n",
    "# execute the LLM query, with automatic JSON validation+retry\n",
    "parsed_response, raw_response, error = llm.get_json_response(prompt=json_prompt, json_validation_desc=json_output_spec)\n",
    "\n",
    "# save and report results\n",
    "if error:\n",
    "    print()\n",
    "    print(f\"Error: {error}\")\n",
    "\n",
    "    codes = []\n",
    "else:\n",
    "    codes = parsed_response['codes']\n",
    "    print()\n",
    "    print(f\"Proposed {len(codes)} codes:\")\n",
    "    for code in codes:\n",
    "        print()\n",
    "        print(f\"  * {code['id']}: {code['label']}\")\n",
    "\n",
    "# next, if we have a proposed set of codes, try to refine them\n",
    "if codes:\n",
    "    # re-randomize the list of transcript summaries\n",
    "    all_summaries = get_all_summaries(documents)\n",
    "\n",
    "    print()\n",
    "    print(f\"Creating first-draft codebook...\")\n",
    "\n",
    "    # describe the job we want the LLM to do\n",
    "    job_description = \"\"\"I have a draft codebook derived from a set of interview summaries, and your job is to refine and finalize that codebook. Each code currently includes a unique ID, a label, a definition, and an example excerpt. I'd like you to refine this codebook based on the content and patterns described in these summaries. Specifically:\n",
    "\n",
    "1. Review each code and confirm that it is clearly defined and relevant to the summarized data.\n",
    "2. If any codes overlap significantly or appear redundant, merge or rename them to reduce redundancy.\n",
    "3. If any codes are too broad, vague, or ambiguous, modify them as appropriate (e.g., split them into more specific codes or clarify definitions).\n",
    "4. Identify any gaps (areas where a code might be missing because the summaries hint at concepts or themes not yet represented). Add new codes if needed.\n",
    "5. Update the examples to ensure they are representative and clearly illustrate when to apply the code.\n",
    "\n",
    "Please return a revised codebook that best represents the recurring themes and nuances of the summaries — and be sure to keep the final codebook that you return anchored in the data as presented by the summaries. Do not introduce concepts not supported by the summaries.\"\"\"\n",
    "\n",
    "    # (we'll use the same json_output_spec from earlier since we want the same format back from the LLM)\n",
    "\n",
    "    # assemble the overall prompt\n",
    "    json_prompt = f\"\"\"{job_description}\n",
    "\n",
    "{json_output_spec}\n",
    "\n",
    "**Transcript summaries:**\n",
    "All transcript summaries enclosed in a code block:\n",
    "\n",
    "```\n",
    "{all_summaries}\n",
    "```\n",
    "\n",
    "**Draft codebook:**\n",
    "The draft codebook, in JSON format, enclosed in a code block:\n",
    "```\n",
    "{json.dumps(parsed_response, indent=2)}\n",
    "```\n",
    "\n",
    "Your JSON response precisely following the instructions given above the summaries and draft:\"\"\"\n",
    "\n",
    "    # execute the LLM query, with automatic JSON validation+retry\n",
    "    parsed_response, raw_response, error = llm.get_json_response(prompt=json_prompt, json_validation_desc=json_output_spec)\n",
    "\n",
    "    # save and report results\n",
    "    if error:\n",
    "        print()\n",
    "        print(f\"Error: {error}\")\n",
    "\n",
    "        codes = []\n",
    "    else:\n",
    "        codes = parsed_response['codes']\n",
    "        print()\n",
    "        print(f\"Final draft includes {len(codes)} codes:\")\n",
    "        for code in codes:\n",
    "            print()\n",
    "            print(f\"  * {code['id']}: {code['label']}\")"
   ],
   "metadata": {
    "collapsed": true,
    "id": "XfEULWNH_2_i"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Coding each document, extracting excerpts\n",
    "\n",
    "This next code block will code each document according to the codebook, identifying a relevant excerpt to go with each code. Feel free to adjust the instructions used to guide the coding and excerpt extraction to meet your needs.\n",
    "\n",
    "If we wanted this to be faster and cheaper, we could code documents based on their summaries rather than going back to their full text. However, going back to the full text allows us to be more thorough — and it allows us to extract actual excerpts from the original text."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# if no themes identified, raise an error\n",
    "if not codes:\n",
    "    raise Exception(\"No codes identified, so we can't code the documents\")\n",
    "\n",
    "# loop through each document\n",
    "error_count = 0\n",
    "last_error = \"\"\n",
    "for document in documents:\n",
    "    print()\n",
    "    print(f\"Coding {document['file']}...\")\n",
    "\n",
    "    # get document text and truncate if needed\n",
    "    doc_text = document['text']\n",
    "    doc_tokens = llm.count_tokens(doc_text)\n",
    "    if doc_tokens > max_doc_tokens:\n",
    "        doc_text = llm.enforce_max_tokens(doc_text, max_doc_tokens)\n",
    "        print(f\"  Warning: only considering first {max_doc_tokens} of {doc_tokens} tokens in document\")\n",
    "\n",
    "    # describe the job we want the LLM to do\n",
    "    job_description = f\"\"\"You are given a single interview transcript and a codebook (defined below). Your task is to read the transcript carefully and then apply those codes from the codebook that apply to the transcript. For each code that you apply to the transcript, you should:\n",
    "\n",
    "1. Supply the unique `id` for the code\n",
    "2. Supply an `excerpt` from the transcript that best supports the application of the code\"\"\"\n",
    "\n",
    "    # describe the exact JSON format we expect back from the LLM\n",
    "    json_output_spec = \"\"\"Return JSON with the following fields (and only the following fields):\n",
    "\n",
    "* `codes` (list of objects): The list of ALL codes from the codebook that apply to the transcript (be sure not to miss any), each of which should be an object with the following fields:\n",
    "\n",
    "    * `id` (string): The short identifier for code from the codebook. Must exactly match an `id` from the codebook.\n",
    "\n",
    "    * `excerpt` (string): An excerpt from the transcript that best supports the application of the code. This should be a short, relevant snippet of text that clearly demonstrates why the code applies to the transcript. Do not paraphrase or alter the text in any way, except as necessary to format in proper JSON format.\"\"\"\n",
    "\n",
    "    # assemble the overall prompt\n",
    "    json_prompt = f\"\"\"{job_description}\n",
    "\n",
    "{json_output_spec}\n",
    "\n",
    "**Codebook:**\n",
    "The codebook in JSON format, enclosed in a code block:\n",
    "```\n",
    "{json.dumps(codes, indent=2)}\n",
    "```\n",
    "\n",
    "**Interview transcript:**\n",
    "The full interview transcript, enclosed in a code block:\n",
    "```\n",
    "{doc_text}\n",
    "```\n",
    "\n",
    "Your JSON response precisely following the instructions given above the transcript and codebook:\"\"\"\n",
    "\n",
    "    # execute the LLM query, with automatic JSON validation+retry\n",
    "    parsed_response, raw_response, error = llm.get_json_response(prompt=json_prompt, json_validation_desc=json_output_spec)\n",
    "\n",
    "    # save and report results\n",
    "    if error:\n",
    "        document['codes'] = \"\"\n",
    "\n",
    "        print(f\"  Error during coding: {error}\")\n",
    "\n",
    "        error_count += 1\n",
    "        last_error = error\n",
    "    else:\n",
    "        document['codes'] = parsed_response['codes']\n",
    "\n",
    "        print(f\"  Coded with: {', '.join([code['id'] for code in parsed_response['codes']])}\")\n",
    "\n",
    "# report overall results\n",
    "if error_count > 0:\n",
    "    print()\n",
    "    print(f\"Some documents could not be coded due to errors.\")\n",
    "    print(f\"  * Total errors encountered: {error_count}\")\n",
    "    print(f\"  * Last error encountered: {last_error}\")\n",
    "else:\n",
    "    print()\n",
    "    print(f\"All {len(documents)} documents coded successfully.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Organizing and outputting the results\n",
    "\n",
    "This final code block organizes and outputs final results, saving them in two `.csv` files:\n",
    "\n",
    "1. `example-qual-analysis-1-codebook.csv` - The codebook used to code the transcripts\n",
    "2. `example-qual-analysis-1-documents.csv` - A wide-format `.csv` file with dummy variables and excerpts for all codes\n",
    "\n",
    "If you're running in Google Colab, these files will be saved into the content folder. Find, view, and download them by clicking on the folder icon in the left sidebar.\n",
    "\n",
    "If you're running elsewhere, they will be saved into an `ai-workflows` subdirectory created off of your user home directory."
   ],
   "metadata": {
    "id": "trJ9EDFCWQrR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# output files to ~/ai-workflows directory if local, otherwise /content if Google Colab\n",
    "output_path_prefix = notebook_env.get_output_dir(not_colab_dir=\"~/ai-workflows\", colab_subdir=\"\")\n",
    "\n",
    "# output theme list to UTF-8 .csv file\n",
    "codebook_output_file = os.path.join(output_path_prefix, \"example-qual-analysis-1-codebook.csv\")\n",
    "with open(codebook_output_file, \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"label\", \"definition\", \"example\"])\n",
    "    for code in codes:\n",
    "        writer.writerow([code['id'], code['label'], code['definition'], code['example']])\n",
    "\n",
    "# output document list to UTF-8 .csv file in wide format\n",
    "docs_output_file = os.path.join(output_path_prefix, \"example-qual-analysis-1-documents.csv\")\n",
    "# output two columns for each code (one for presence, one for excerpt)\n",
    "code_columns = []\n",
    "for code in codes:\n",
    "    code_columns.append(f\"{code['id']}\")\n",
    "    code_columns.append(f\"{code['id']}_excerpt\")\n",
    "\n",
    "with open(docs_output_file, \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write header\n",
    "    writer.writerow([\"file\", \"summary\"] + code_columns)\n",
    "\n",
    "    # write document data\n",
    "    for document in documents:\n",
    "        # assemble values for code columns\n",
    "        code_values = []\n",
    "        for code in codes:\n",
    "            code_id = code['id']\n",
    "            code_present = 1 if code_id in [c['id'] for c in document['codes']] else 0\n",
    "            code_excerpt = next((c['excerpt'] for c in document['codes'] if c['id'] == code_id), \"\")\n",
    "            code_values.extend([code_present, code_excerpt])\n",
    "\n",
    "        # write row for document\n",
    "        writer.writerow([\n",
    "            document['file'],\n",
    "            document['summary'],\n",
    "        ] + code_values)\n",
    "\n",
    "print()\n",
    "print(f\"Codebook saved to {codebook_output_file}\")\n",
    "print(f\"Transcript summaries, codes, and excerpts saved to {docs_output_file}\")"
   ],
   "metadata": {
    "id": "wqn1yzYsWfq5"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
