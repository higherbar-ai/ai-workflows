{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/higherbar-ai/ai-workflows/blob/main/src/example-doc-extraction-templated.ipynb\" target=\"_parent\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>\n",
    "\n",
    "# About this notebook\n",
    "\n",
    "This notebook provides an example of how the `ai-workflows` package can be used to extract arbitrary structured data from an arbitrary number of unstructured documents. It prompts you to upload an `.xlsx` file with a data extraction template, then one or more files from which to extract data. The template follows a specific format, as in [the example template here](https://docs.google.com/spreadsheets/d/1jI0hXKS_vPlbaFH-wwYTVBVYShjDS4m3N14fU21JfK4/edit) (which you can copy and edit to adapt to your needs).\n",
    "\n",
    "## Configuration\n",
    "\n",
    "This notebook requires different settings depending on which AI service providers you want to use. If you're running in Google Colab, you configure these settings as \"secrets\"; just click the key icon in the left sidebar (and, once you create a secret, be sure to click the toggle to give the notebook access to the secret). If you're running this notebook in a different environment, you can set these settings in a `.env` file; the first time you run, it will write out a template `.env` file for you to fill in and direct you to its location. \n",
    "\n",
    "Following are the settings, regardless of the environment.\n",
    "\n",
    "### OpenAI (direct)\n",
    "\n",
    "To use OpenAI directly:\n",
    "\n",
    "* `openai_api_key` - your OpenAI API key (get one from [the OpenAI API key page](https://platform.openai.com/api-keys), and be sure to fund your platform account with at least $5 to allow GPT-4o model access)\n",
    "* `openai_model` (optional) - the model to use (defaults to `gpt-4o`)\n",
    "\n",
    "### OpenAI (via Microsoft Azure)\n",
    "\n",
    "To use OpenAI via Microsoft Azure:\n",
    "\n",
    "* `azure_api_key` - your Azure API key\n",
    "* `azure_api_base` - the base URL for the Azure API\n",
    "* `azure_api_engine` - the engine to use (a.k.a. the \"deployment\")\n",
    "* `azure_api_version` - the API version to use\n",
    "\n",
    "### Anthropic (direct)\n",
    "\n",
    "To use Anthropic directly:\n",
    "\n",
    "* `anthropic_api_key` - your Anthropic API key\n",
    "* `anthropic_model` - the model to use\n",
    "\n",
    "### LangSmith (for tracing)\n",
    "\n",
    "Optionally, you can add [LangSmith tracing](https://langchain.com/langsmith):\n",
    "\n",
    "* `langsmith_api_key` - your LangSmith API key\n",
    "\n",
    "## Setting up the runtime environment\n",
    "\n",
    "This next code block installs all necessary Python and system packages into the current environment.\n",
    "\n",
    "**If you're running in Google Colab and it prompts you to restart the notebook in the middle of the installation steps, just click CANCEL.**"
   ],
   "id": "3131c033cc6ec753"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# install Google Colab Support and ai_workflows package\n",
    "%pip install colab-or-not py-ai-workflows[docs]\n",
    "\n",
    "# download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', force=True)\n",
    "\n",
    "# set up our notebook environment (including LibreOffice)\n",
    "from colab_or_not import NotebookBridge\n",
    "notebook_env = NotebookBridge(\n",
    "    system_packages=[\"libreoffice\"],\n",
    "    config_path=\"~/.hbai/ai-workflows.env\",\n",
    "    config_template={\n",
    "        \"openai_api_key\": \"\",\n",
    "        \"openai_model\": \"\",\n",
    "        \"azure_api_key\": \"\",\n",
    "        \"azure_api_base\": \"\",\n",
    "        \"azure_api_engine\": \"\",\n",
    "        \"azure_api_version\": \"\",\n",
    "        \"anthropic_api_key\": \"\",\n",
    "        \"anthropic_model\": \"\",\n",
    "        \"langsmith_api_key\": \"\",\n",
    "    }\n",
    ")\n",
    "notebook_env.setup_environment()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initializing for AI workflows\n",
    "\n",
    "The next code block initializes the notebook by loading settings and initializing the LLM interface."
   ],
   "id": "343a69892d52b765"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ai_workflows.llm_utilities import LLMInterface\n",
    "from ai_workflows.document_utilities import DocumentInterface\n",
    "\n",
    "# read all supported settings\n",
    "openai_api_key = notebook_env.get_setting('openai_api_key')\n",
    "openai_model = notebook_env.get_setting('openai_model', 'gpt-4o')\n",
    "azure_api_key = notebook_env.get_setting('azure_api_key')\n",
    "azure_api_base = notebook_env.get_setting('azure_api_base')\n",
    "azure_api_engine = notebook_env.get_setting('azure_api_engine')\n",
    "azure_api_version = notebook_env.get_setting('azure_api_version')\n",
    "anthropic_api_key = notebook_env.get_setting(\"anthropic_api_key\")\n",
    "anthropic_model = notebook_env.get_setting(\"anthropic_model\")\n",
    "langsmith_api_key = notebook_env.get_setting('langsmith_api_key')\n",
    "\n",
    "# complain if we don't have the bare minimum to run\n",
    "if (not openai_api_key\n",
    "        and not (azure_api_key and azure_api_base and azure_api_engine and azure_api_version)\n",
    "        and not (anthropic_api_key and anthropic_model)):\n",
    "    raise Exception('We need settings set for OpenAI access (direct or via Azure) or for Anthropic access (direct). See the instructions above for more details.')\n",
    "\n",
    "# initialize LLM interface\n",
    "llm = LLMInterface(openai_api_key=openai_api_key, openai_model=openai_model, azure_api_key=azure_api_key, azure_api_base=azure_api_base, azure_api_engine=azure_api_engine, azure_api_version=azure_api_version, temperature = 0.0, total_response_timeout_seconds=600, number_of_retries=2, seconds_between_retries=5, langsmith_api_key=langsmith_api_key, anthropic_api_key=anthropic_api_key, anthropic_model=anthropic_model)\n",
    "\n",
    "# initialize our document processor\n",
    "doc_interface = DocumentInterface(llm_interface=llm)\n",
    "\n",
    "# report success\n",
    "print(\"Initialization successful.\")"
   ],
   "id": "80a6929f49e358bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompting for input files\n",
    "\n",
    "This next code block prompts you to upload or select your input files:\n",
    "\n",
    "1. An `.xlsx` file with the document extraction template. This template specifies information about the document(s) you want to process and the information you wish to extract from those documents; it follows a very specific format, as in [the example template here](https://docs.google.com/spreadsheets/d/1jI0hXKS_vPlbaFH-wwYTVBVYShjDS4m3N14fU21JfK4/edit). The easiest thing to do is (a) open [the example template](https://docs.google.com/spreadsheets/d/1jI0hXKS_vPlbaFH-wwYTVBVYShjDS4m3N14fU21JfK4/edit), (b) click on \"File\" > \"Make a copy\" to make your own copy, and (c) edit the template to meet your needs, (d) click on \"File\" > \"Download\" > \"Microsoft Excel (.xlsx)\" to download the template to your computer, and (e) upload or select the template here.\n",
    "\n",
    "2. The document(s) you want to process. If you only want to process a single document, just upload or select that. If you want to process multiple, upload or select them all or compress them together into a single `.zip` file and upload or select that."
   ],
   "id": "94ead9d4f72c8368"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "template_file = \"\"\n",
    "while True:\n",
    "    # prompt for a .xlsx file\n",
    "    selected_files = notebook_env.get_input_files(\".xlsx file with processing template:\")\n",
    "\n",
    "    # complain if we didn't get just a single .xlsx file\n",
    "    if len(selected_files) != 1 or not selected_files[0].lower().endswith('.xlsx'):\n",
    "        print()\n",
    "        print('Please upload a single .xlsx file with your processing template to continue.')\n",
    "        print()\n",
    "    else:\n",
    "        # fetch the path of the uploaded file\n",
    "        template_file = selected_files[0]\n",
    "\n",
    "        # break from loop\n",
    "        break\n",
    "\n",
    "# report out on the processing template\n",
    "print(f'Will process using template: {template_file}')\n",
    "\n",
    "# prompt for one or more files to process\n",
    "files_to_process = notebook_env.get_input_files(\"Document(s) to process (.zip file for multiple):\")\n",
    "\n",
    "# report out on the files we plan to process\n",
    "for file_to_process in files_to_process:\n",
    "    if file_to_process.lower().endswith('.zip'):\n",
    "        print(f'Will process all files within: {file_to_process}')\n",
    "    else:\n",
    "        print(f'Will process: {file_to_process}')"
   ],
   "id": "bdae454b5dc58584",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extracting data\n",
    "\n",
    "This next code block starts by reading the supplied template to guide the data extraction process. It then processes each file one-by-one, unzipping `.zip` files into a temporary directory as needed, and aggregates all the results into a single list of rows."
   ],
   "id": "81b4ca99048e30b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tempfile\n",
    "import zipfile\n",
    "import os\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# load the template from the first worksheet in the uploaded workbook\n",
    "wb = load_workbook(template_file)\n",
    "sheet = wb.worksheets[0]\n",
    "\n",
    "# extract the template from the first two columns of the worksheet\n",
    "template_dict = {}\n",
    "for row in sheet.iter_rows(values_only=True):\n",
    "    # extract the first two columns\n",
    "    key_cell = row[0] if len(row) > 0 else None\n",
    "    val_cell = row[1] if len(row) > 1 else None\n",
    "\n",
    "    # only proceed if both key and value cells have content â€” and if it's not a duplicate key\n",
    "    if key_cell and val_cell:\n",
    "        key = str(key_cell).strip()\n",
    "        value = str(val_cell).strip()\n",
    "        if key not in template_dict:\n",
    "            template_dict[key] = value\n",
    "\n",
    "# organize template data for processing\n",
    "if 'meta_document_desc' not in template_dict:\n",
    "    raise Exception('Processing template must include a \"meta_document_desc\" row that describes the documents being processed.')\n",
    "document_desc = template_dict['meta_document_desc']\n",
    "if 'meta_row_desc' not in template_dict:\n",
    "    raise Exception('Processing template must include a \"meta_row_desc\" row that describes the rows of data you want extracted.')\n",
    "row_desc = template_dict['meta_row_desc']\n",
    "page_by_page = False\n",
    "if 'meta_opt_page_by_page' in template_dict:\n",
    "    page_by_page = (template_dict['meta_opt_page_by_page'] == '1' or template_dict['meta_opt_page_by_page'].lower() == 'true')\n",
    "output_data = {}\n",
    "for k, v in template_dict.items():\n",
    "    if not k.startswith(\"meta_\"):\n",
    "        output_data[k] = v\n",
    "\n",
    "# next, process files, with zip files unzipped into a temporary directory\n",
    "all_rows = []\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # tally up all files, unzipping as needed\n",
    "    all_files = []\n",
    "    for file_to_process in files_to_process:\n",
    "        if file_to_process.lower().endswith('.zip'):\n",
    "            # if it's a .zip file, unzip it into the temporary directory\n",
    "            print(f'Unzipping {file_to_process}')\n",
    "            with zipfile.ZipFile(file_to_process, 'r') as zip_ref:\n",
    "                zip_ref.extractall(temp_dir)\n",
    "        else:\n",
    "            # just add the file to the list of files to process\n",
    "            all_files.append(file_to_process)\n",
    "    # add all unzipped files to the list of files to process (ignoring hidden files)\n",
    "    for root, dirs, files in os.walk(temp_dir):\n",
    "        for unzipped_file in files:\n",
    "            unzipped_file_path = os.path.join(root, unzipped_file)\n",
    "            if not unzipped_file.startswith('.'):\n",
    "                all_files.append(unzipped_file_path)\n",
    "\n",
    "    # process each file\n",
    "    for file_to_process in all_files:\n",
    "        filename = os.path.basename(file_to_process)\n",
    "        print(f'Processing {filename}...')\n",
    "\n",
    "        json_context = f\"\"\"The file contains the following: {document_desc}\"\"\"\n",
    "\n",
    "        json_job = f\"\"\"Your job is to extract a series of row objects from the file's content, and to return them all in a specific JSON format. Each row object should represent the following: {row_desc}\"\"\"\n",
    "\n",
    "        json_output_spec = f\"\"\"Return JSON with the following fields (and only the following fields):\\n\\n* `rows` (list): The list of rows extracted, or an empty list if none found. Each row should contain the following fields:\"\"\"\n",
    "        for k, v in output_data.items():\n",
    "            json_output_spec += f\"\"\"\\n\\n  * `{k}` (string): {v}\"\"\"\n",
    "\n",
    "        # process the file\n",
    "        all_responses = doc_interface.convert_to_json(file_to_process, json_context, json_job, json_output_spec, markdown_first=not page_by_page)\n",
    "\n",
    "        # combine all responses into a single list of rows\n",
    "        merged_responses = doc_interface.merge_dicts(all_responses)\n",
    "        rows = merged_responses['rows']\n",
    "\n",
    "        # output and save results\n",
    "        print(f\"  Extracted {len(rows)} row{'s' if len(rows) != 1 else ''}\")\n",
    "        all_rows.append((filename, rows))"
   ],
   "id": "6ff4e0f917b555cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Outputting extracted data\n",
    "\n",
    "This final code block outputs the extracted data to `extracted_data.csv`, with the filename in column A and the requested output columns thereafter.\n",
    "\n",
    "If you're running in Google Colab, this `.csv` file will be saved into the content folder. Find, view, or download it by clicking on the folder icon in the left sidebar.\n",
    "\n",
    "If you're running elsewhere, it will be saved into an `ai-workflows` subdirectory created off of your user home directory."
   ],
   "id": "29bf70b43f324e62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "# output files to ~/ai-workflows directory if local, otherwise /content if Google Colab\n",
    "output_path_prefix = notebook_env.get_output_dir(not_colab_dir=\"~/ai-workflows\", colab_subdir=\"\")\n",
    "\n",
    "# output .csv file with extracted data, with filename in column A and the output columns thereafter\n",
    "output_csv_path = os.path.join(output_path_prefix, 'extracted_data.csv')\n",
    "with open(output_csv_path, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['filename'] + list(output_data.keys()))\n",
    "    for filename, rows in all_rows:\n",
    "        for row in rows:\n",
    "            csvwriter.writerow([filename] + [row.get(k, '') for k in output_data.keys()])\n",
    "\n",
    "# report out on the output file\n",
    "print(f'Extracted data saved to: {output_csv_path}')"
   ],
   "id": "79a0149bcb008083",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
