{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Example testing notebook\n",
    "\n",
    "This notebook provides an example of how you can test LLM-assisted document conversion vs. the alternative. Before attempting to run, be sure to set up your Python environment using the code in `initial-setup.ipynb` and configure the `.ini` file as discussed below.\n",
    "\n",
    "The notebook begins by loading credentials and configuration from an `.ini` file stored in `~/.hbai/ai-workflows.ini`. The `~` in the path refers to the current user's home directory, and the `.ini` file contents should follow this format (with keys, models, and paths as appropriate):\n",
    "\n",
    "    [openai]\n",
    "    openai-api-key=keyhere-with-sk-on-front\n",
    "    openai-model=gpt-4o\n",
    "    azure-api-key=keyhere-or-blank\n",
    "    azure-api-base=azure-base-url-here\n",
    "    azure-api-engine=gpt-4o\n",
    "    azure-api-version=2024-02-01\n",
    "\n",
    "    [langsmith]\n",
    "    langsmith-api-key=leave-blank-unless-you're-using-langsmith\n",
    "\n",
    "    [files]\n",
    "    input-dir=~/Files/ai-workflows/inputs\n",
    "    output-dir=~/Files/ai-workflows/outputs\n",
    "\n",
    "You can leave the Azure settings blank if you're using OpenAI (or vice versa). You also don't need to supply a Langsmith API key unless you're using Langsmith. The `input-dir` and `output-dir` settings are used to specify the directories where input and output files are stored, respectively.\n",
    "\n",
    "## Initializing\n",
    "\n",
    "This next code block initializes the notebook, reading parameters from the configuration file and initializing an LLM interface."
   ],
   "id": "8b6627c85f89ca21"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-29T13:52:07.124979Z",
     "start_time": "2024-10-29T13:52:05.482904Z"
    }
   },
   "source": [
    "# for convenience, auto-reload modules when they've changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "import configparser\n",
    "import os\n",
    "from ai_workflows.llm_utilities import LLMInterface \n",
    "\n",
    "# set log level to WARNING\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# load credentials and other configuration from a local ini file\n",
    "inifile_location = os.path.expanduser(\"~/.hbai/ai-workflows.ini\")\n",
    "inifile = configparser.RawConfigParser()\n",
    "inifile.read(inifile_location)\n",
    "\n",
    "# load configuration\n",
    "openai_api_key = inifile.get(\"openai\", \"openai-api-key\")\n",
    "openai_model = inifile.get(\"openai\", \"openai-model\")\n",
    "azure_api_key = inifile.get(\"openai\", \"azure-api-key\")\n",
    "azure_api_base = inifile.get(\"openai\", \"azure-api-base\")\n",
    "azure_api_engine = inifile.get(\"openai\", \"azure-api-engine\")\n",
    "azure_api_version = inifile.get(\"openai\", \"azure-api-version\")\n",
    "input_dir = os.path.expanduser(inifile.get(\"files\", \"input-dir\"))\n",
    "output_dir = os.path.expanduser(inifile.get(\"files\", \"output-dir\"))\n",
    "langsmith_api_key = inifile.get(\"langsmith\", \"langsmith-api-key\")\n",
    "\n",
    "# initialize LangSmith API (if key specified)\n",
    "if langsmith_api_key:\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"local\"\n",
    "    os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = langsmith_api_key\n",
    "\n",
    "# initialize the LLM\n",
    "llm = LLMInterface(\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_model=openai_model,\n",
    "    azure_api_key=azure_api_key,\n",
    "    azure_api_base=azure_api_base,\n",
    "    azure_api_engine=azure_api_engine,\n",
    "    azure_api_version=azure_api_version,\n",
    "    langsmith_api_key=langsmith_api_key\n",
    ")\n",
    "\n",
    "# report results\n",
    "print(\"Local configuration loaded, LLM initialized.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local configuration loaded, LLM initialized.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Converting input documents to Markdown\n",
    "\n",
    "This next code block runs through all the files in the configured input directory and converts them to Markdown, saving the Markdown files in the output directory. It also converts the files without the LLM (adding a \"-no-llm\" suffix to the base name of each output file) so that you can see the difference betweeen LLM-assisted and regular conversion."
   ],
   "id": "2d8772f49725280f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T13:52:28.820769Z",
     "start_time": "2024-10-29T13:52:28.804586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# use document_utilities to convert all files in the input directory\n",
    "from ai_workflows.document_utilities import DocumentInterface\n",
    "\n",
    "# initialize the document interface\n",
    "doc = DocumentInterface(llm_interface=llm)\n",
    "doc_no_llm = DocumentInterface()\n",
    "\n",
    "# convert all files in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if os.path.isfile(os.path.join(input_dir, filename)) and not filename.startswith('.') and not filename.endswith('.md'):\n",
    "        print(f\"Converting {filename} to markdown...\")\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        markdown = doc.convert_to_markdown(input_path)\n",
    "\n",
    "        # write the markdown to the output directory\n",
    "        output_path = os.path.join(output_dir, os.path.splitext(os.path.basename(filename))[0] + '.md')\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(markdown)\n",
    "\n",
    "        # now convert again without the LLM\n",
    "        markdown_no_llm = doc_no_llm.convert_to_markdown(input_path)\n",
    "        output_path = os.path.join(output_dir, os.path.splitext(os.path.basename(filename))[0] + '-no-llm.md')\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(markdown_no_llm)        \n",
    "\n",
    "        print(f\"Conversion complete. Markdown saved to {output_path}\")"
   ],
   "id": "a93b4c932412ba5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting SurveyCTO Team-month Consumption Tracker.xlsx to markdown...\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comparing Markdown documents\n",
    "\n",
    "The next several sections have code to compare the LLM and no-LLM versions of the Markdown outputs. They are a work-in-progress."
   ],
   "id": "d1e02188ca9ac38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "class MarkdownDiffAnalyzer:\n",
    "    \"\"\"A class to analyze differences between Markdown documents while ignoring formatting.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def strip_markdown(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove markdown formatting while preserving the actual text content.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The markdown text to process\n",
    "            \n",
    "        Returns:\n",
    "            str: Text with markdown formatting removed\n",
    "        \"\"\"\n",
    "        \n",
    "        # Remove code blocks and their content\n",
    "        text = re.sub(r'```[\\s\\S]*?```', '', text)\n",
    "        \n",
    "        # Remove inline code\n",
    "        text = re.sub(r'`[^`]*`', '', text)\n",
    "        \n",
    "        # Remove headers\n",
    "        text = re.sub(r'^#{1,6}\\s*', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove bold and italic\n",
    "        text = re.sub(r'\\*\\*.*?\\*\\*', lambda m: m.group()[2:-2], text)\n",
    "        text = re.sub(r'\\*.*?\\*', lambda m: m.group()[1:-1], text)\n",
    "        text = re.sub(r'__.*?__', lambda m: m.group()[2:-2], text)\n",
    "        text = re.sub(r'_.*?_', lambda m: m.group()[1:-1], text)\n",
    "        \n",
    "        # Remove links but keep text\n",
    "        text = re.sub(r'\\[([^\\]]+)\\]\\([^\\)]+\\)', r'\\1', text)\n",
    "        \n",
    "        # Remove images\n",
    "        text = re.sub(r'!\\[([^\\]]*)\\]\\([^\\)]+\\)', '', text)\n",
    "        \n",
    "        # Remove horizontal rules\n",
    "        text = re.sub(r'^[-*_]{3,}$', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove blockquotes\n",
    "        text = re.sub(r'^\\s*>\\s*', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove list markers\n",
    "        text = re.sub(r'^\\s*[-*+]\\s+', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'^\\s*\\d+\\.\\s+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove tables\n",
    "        text = re.sub(r'\\|.*\\|', '', text)\n",
    "        text = re.sub(r'^\\s*[-:|\\s]+$', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_missing_chunks(text1: str, text2: str, min_length: int = 10) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"\n",
    "        Find chunks of text that exist in one document but not the other.\n",
    "        \n",
    "        Args:\n",
    "            text1 (str): First text to compare\n",
    "            text2 (str): Second text to compare\n",
    "            min_length (int): Minimum length of chunks to consider\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[List[str], List[str]]: Lists of chunks unique to text1 and text2\n",
    "        \"\"\"\n",
    "        # Initialize sequence matcher\n",
    "        matcher = SequenceMatcher(None, text1, text2)\n",
    "        \n",
    "        # Get matching blocks\n",
    "        matches = matcher.get_matching_blocks()\n",
    "        \n",
    "        # Find chunks unique to text1\n",
    "        unique_to_1 = []\n",
    "        last_a = 0\n",
    "        for match in matches:\n",
    "            i, j, n = match\n",
    "            if i - last_a >= min_length:\n",
    "                unique_to_1.append(text1[last_a:i].strip())\n",
    "            last_a = i + n\n",
    "            \n",
    "        # Find chunks unique to text2\n",
    "        unique_to_2 = []\n",
    "        last_b = 0\n",
    "        for match in matches:\n",
    "            i, j, n = match\n",
    "            if j - last_b >= min_length:\n",
    "                unique_to_2.append(text2[last_b:j].strip())\n",
    "            last_b = j + n\n",
    "            \n",
    "        return unique_to_1, unique_to_2\n",
    "    \n",
    "    def compare_markdown_docs(self, md1: str, md2: str, min_length: int = 10) -> Dict:\n",
    "        \"\"\"\n",
    "        Compare two Markdown documents and find their differences.\n",
    "        \n",
    "        Args:\n",
    "            md1 (str): First Markdown document\n",
    "            md2 (str): Second Markdown document\n",
    "            min_length (int): Minimum length of different chunks to report\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Dictionary containing analysis results\n",
    "        \"\"\"\n",
    "        # Strip Markdown formatting\n",
    "        clean1 = self.strip_markdown(md1)\n",
    "        clean2 = self.strip_markdown(md2)\n",
    "        \n",
    "        # Find differences\n",
    "        missing_from_2, missing_from_1 = self.find_missing_chunks(clean1, clean2, min_length)\n",
    "        \n",
    "        # Calculate similarity ratio\n",
    "        similarity = SequenceMatcher(None, clean1, clean2).ratio()\n",
    "        \n",
    "        return {\n",
    "            'similarity_ratio': similarity,\n",
    "            'missing_from_doc1': missing_from_1,\n",
    "            'missing_from_doc2': missing_from_2,\n",
    "            'clean_text1': clean1,\n",
    "            'clean_text2': clean2\n",
    "        }"
   ],
   "id": "889206ac9557d25f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import difflib\n",
    "\n",
    "# loop through output files and compare the -no-llm version to the LLM version\n",
    "for filename in os.listdir(output_dir):\n",
    "    if os.path.isfile(os.path.join(output_dir, filename)) and filename.endswith('.md') and not filename.endswith('-no-llm.md'):\n",
    "        print(f\"\\n\\nComparing {filename} to {filename.replace('.md', '-no-llm.md')}...\")\n",
    "        with open(os.path.join(output_dir, filename), 'r') as f:\n",
    "            md1 = f.read()\n",
    "        with open(os.path.join(output_dir, filename.replace('.md', '-no-llm.md')), 'r') as f:\n",
    "            md2 = f.read()\n",
    "        \n",
    "        results = MarkdownDiffAnalyzer().compare_markdown_docs(md1, md2)\n",
    "        \n",
    "        print(f\"Similarity ratio: {results['similarity_ratio']:.2%}\")\n",
    "        print(\"\\nMissing from document 1 (LLM version):\")\n",
    "        for chunk in results['missing_from_doc1']:\n",
    "            print(f\"- {chunk}\")\n",
    "\n",
    "        print(\"\\nMissing from document 2 (no-LLM version):\")\n",
    "        for chunk in results['missing_from_doc2']:\n",
    "            print(f\"- {chunk}\")\n",
    "        \n",
    "        print(\"\\nDifferences:\")\n",
    "        for line in difflib.unified_diff(md1.splitlines(), md2.splitlines(), fromfile='With LLM', tofile='Without LLM', lineterm=''):\n",
    "            print(line)"
   ],
   "id": "cb34052874464dc9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
