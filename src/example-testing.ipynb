{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Example testing notebook\n",
    "\n",
    "This notebook provides an example of how you can test LLM-assisted document conversion vs. the alternative. Before attempting to run, be sure to set up your Python environment using the code in `initial-setup.ipynb` and configure the `.ini` file as discussed below.\n",
    "\n",
    "The notebook begins by loading credentials and configuration from an `.ini` file stored in `~/.hbai/ai-workflows.ini`. The `~` in the path refers to the current user's home directory, and the `.ini` file contents should follow this format (with keys, models, and paths as appropriate):\n",
    "\n",
    "    [openai]\n",
    "    openai-api-key=keyhere-with-sk-on-front\n",
    "    openai-model=gpt-4o\n",
    "    azure-api-key=keyhere-or-blank\n",
    "    azure-api-base=azure-base-url-here\n",
    "    azure-api-engine=gpt-4o\n",
    "    azure-api-version=2024-02-01\n",
    "\n",
    "    [anthropic]\n",
    "    anthropic-api-key=keyhere\n",
    "    anthropic-model=\n",
    "\n",
    "    [aws]\n",
    "    aws-profile=\n",
    "    bedrock-model=\n",
    "    bedrock-region=us-east-1\n",
    "\n",
    "    [langsmith]\n",
    "    langsmith-api-key=leave-blank-unless-you're-using-langsmith\n",
    "\n",
    "    [files]\n",
    "    input-dir=~/Files/ai-workflows/inputs\n",
    "    output-dir=~/Files/ai-workflows/outputs\n",
    "\n",
    "You can set up either OpenAI, Azure, Anthropic, or Bedrock as the LLM, leaving settings for the other LLMs blank. You also don't need to supply a Langsmith API key unless you're using Langsmith. The `input-dir` and `output-dir` settings are used to specify the directories where input and output files are stored, respectively.\n",
    "\n",
    "## Installing dependencies\n",
    "\n",
    "This next code block installs the dependencies required for this notebook."
   ],
   "id": "8b6627c85f89ca21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# install requirements specific to this notebook (only need to run once in a given environment)\n",
    "%pip install pandas rapidfuzz"
   ],
   "id": "bce858aa2decda1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initializing\n",
    "\n",
    "This next code block initializes the notebook, reading parameters from the configuration file and initializing an LLM interface."
   ],
   "id": "fadda0abd86e1dbb"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# for convenience, auto-reload modules when they've changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "import configparser\n",
    "import os\n",
    "from ai_workflows.llm_utilities import LLMInterface \n",
    "\n",
    "# set log level to WARNING\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# load credentials and other configuration from a local ini file\n",
    "inifile_location = os.path.expanduser(\"~/.hbai/ai-workflows.ini\")\n",
    "inifile = configparser.RawConfigParser()\n",
    "inifile.read(inifile_location)\n",
    "\n",
    "# load configuration\n",
    "openai_api_key = inifile.get(\"openai\", \"openai-api-key\")\n",
    "openai_model = inifile.get(\"openai\", \"openai-model\")\n",
    "azure_api_key = inifile.get(\"openai\", \"azure-api-key\")\n",
    "azure_api_base = inifile.get(\"openai\", \"azure-api-base\")\n",
    "azure_api_engine = inifile.get(\"openai\", \"azure-api-engine\")\n",
    "azure_api_version = inifile.get(\"openai\", \"azure-api-version\")\n",
    "anthropic_api_key = inifile.get(\"anthropic\", \"anthropic-api-key\")\n",
    "anthropic_model = inifile.get(\"anthropic\", \"anthropic-model\")\n",
    "aws_profile = inifile.get(\"aws\", \"aws-profile\")\n",
    "bedrock_model = inifile.get(\"aws\", \"bedrock-model\")\n",
    "bedrock_region = inifile.get(\"aws\", \"bedrock-region\")\n",
    "input_dir = os.path.expanduser(inifile.get(\"files\", \"input-dir\"))\n",
    "output_dir = os.path.expanduser(inifile.get(\"files\", \"output-dir\"))\n",
    "langsmith_api_key = inifile.get(\"langsmith\", \"langsmith-api-key\")\n",
    "\n",
    "# initialize LangSmith API (if key specified)\n",
    "if langsmith_api_key:\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"local\"\n",
    "    os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = langsmith_api_key\n",
    "\n",
    "# initialize the LLM\n",
    "llm = LLMInterface(\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_model=openai_model,\n",
    "    azure_api_key=azure_api_key,\n",
    "    azure_api_base=azure_api_base,\n",
    "    azure_api_engine=azure_api_engine,\n",
    "    azure_api_version=azure_api_version,\n",
    "    langsmith_api_key=langsmith_api_key,\n",
    "    anthropic_api_key=anthropic_api_key,\n",
    "    anthropic_model=anthropic_model,\n",
    "    bedrock_model=bedrock_model,\n",
    "    bedrock_region=bedrock_region,\n",
    "    bedrock_aws_profile=aws_profile\n",
    ")\n",
    "\n",
    "# report results\n",
    "print(\"Local configuration loaded, LLM initialized.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Converting input documents to Markdown\n",
    "\n",
    "This next code block runs through all the files in the configured input directory and converts them to Markdown, saving the Markdown files in the output directory. It also converts the files without the LLM (adding a \"-no-llm\" suffix to the base name of each output file) so that you can see the difference betweeen LLM-assisted and regular conversion."
   ],
   "id": "2d8772f49725280f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# use document_utilities to convert all files in the input directory\n",
    "from ai_workflows.document_utilities import DocumentInterface\n",
    "\n",
    "# initialize the document interface\n",
    "doc = DocumentInterface(llm_interface=llm)\n",
    "doc_no_llm = DocumentInterface()\n",
    "\n",
    "# convert all files in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if os.path.isfile(os.path.join(input_dir, filename)) and not filename.startswith('.') and not filename.endswith('.md'):\n",
    "        print(f\"Converting {filename} to markdown...\")\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        markdown = doc.convert_to_markdown(input_path)\n",
    "\n",
    "        # write the markdown to the output directory\n",
    "        output_path = os.path.join(output_dir, os.path.splitext(os.path.basename(filename))[0] + '.md')\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(markdown)\n",
    "\n",
    "        # now convert again without the LLM\n",
    "        markdown_no_llm = doc_no_llm.convert_to_markdown(input_path)\n",
    "        output_path = os.path.join(output_dir, os.path.splitext(os.path.basename(filename))[0] + '-no-llm.md')\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(markdown_no_llm)        \n",
    "\n",
    "        print(f\"Conversion complete. Markdown saved to {output_path}\")"
   ],
   "id": "a93b4c932412ba5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Customized version of \"the Laterite method\" — thanks, Laterite!\n",
    "\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz, process\n",
    "import unicodedata\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalizes text by:\n",
    "    - Converting to NFC form.\n",
    "    - Replacing specific problematic characters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # normalize Unicode characters to NFC form\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    # replace specific problematic characters\n",
    "    replacements = {\n",
    "        \"â€™\": \"'\",  # Right single quotation mark\n",
    "        \"â€œ\": '\"',  # Left double quotation mark\n",
    "        \"â€\": '\"',  # Right double quotation mark\n",
    "        \"â€“\": '-',  # En dash\n",
    "        \"â€”\": '-',  # Em dash\n",
    "        \"…\": '...',  # Ellipsis\n",
    "    }\n",
    "    for wrong, correct in replacements.items():\n",
    "        text = text.replace(wrong, correct)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def segment_text(text, segment_type='paragraph'):\n",
    "    \"\"\"\n",
    "    Segments text into paragraphs or sentences and ensures each segment has at least two words.\n",
    "    \"\"\"\n",
    "    \n",
    "    if segment_type == 'sentence':\n",
    "        segments = sent_tokenize(text)\n",
    "    elif segment_type == 'paragraph':\n",
    "        paragraphs = re.split(r'\\n{2,}', text)\n",
    "        if len(paragraphs) == 1:\n",
    "            paragraphs = text.split('\\n')\n",
    "        segments = [para.strip() for para in paragraphs if para.strip()]\n",
    "    else:\n",
    "        raise ValueError(\"segment_type must be 'sentence' or 'paragraph'\")\n",
    "    \n",
    "    # filter segments to include only those with at least two words\n",
    "    filtered_segments = [seg for seg in segments if len(seg.split()) >= 2]\n",
    "    return filtered_segments\n",
    "\n",
    "def find_best_match(word_seg, n_grams, similarity_threshold):\n",
    "    \"\"\"\n",
    "    Finds the best matching n-gram above the given similarity threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    match = process.extractOne( # type: ignore\n",
    "        word_seg,\n",
    "        n_grams,\n",
    "        scorer=fuzz.ratio,\n",
    "        score_cutoff=similarity_threshold\n",
    "    )\n",
    "    return match\n",
    "\n",
    "def compare_text(baseline_text: str, comparison_text: str, output_dir: str, base_filename: str):\n",
    "    \"\"\"\n",
    "    Compares comparison_text against baseline_text using tiered similarity thresholds.\n",
    "    \"\"\"\n",
    "    \n",
    "    # start by normalizing both strings\n",
    "    baseline_text = normalize_text(baseline_text)\n",
    "    comparison_text = normalize_text(comparison_text)\n",
    "    \n",
    "    # output normalized text to files\n",
    "    with open(os.path.join(output_dir, f\"{base_filename}-baseline-normalized.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(baseline_text)\n",
    "    with open(os.path.join(output_dir, f\"{base_filename}-comparison-normalized.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(comparison_text)\n",
    "\n",
    "    # segment the baseline text into paragraphs\n",
    "    word_segments = segment_text(baseline_text, segment_type='paragraph')\n",
    "    word_segments = [seg for seg in word_segments if len(seg.split()) >= 2]\n",
    "    print(f\"Number of segments in baseline text (with at least two words): {len(word_segments)}\")\n",
    "\n",
    "    # tokenize the comparison text into words for n-gram generation\n",
    "    comparison_words = comparison_text.split()\n",
    "\n",
    "    # initialize mapping table and track unmatched segments\n",
    "    mapping_table = []\n",
    "    unmatched_segments = [(i + 1, seg) for i, seg in enumerate(word_segments)]\n",
    "    \n",
    "    # define similarity thresholds for tiered matching\n",
    "    similarity_thresholds = [100, 95, 90, 80, 70]\n",
    "    \n",
    "    # process each similarity threshold in turn\n",
    "    for threshold in similarity_thresholds:\n",
    "        if not unmatched_segments:\n",
    "            # if we're done, break out of the loop\n",
    "            break\n",
    "            \n",
    "        still_unmatched = []        \n",
    "        for seg_num, word_seg in unmatched_segments:\n",
    "            # count the number of words in the baseline segment\n",
    "            n = len(word_seg.split())\n",
    "            \n",
    "            # calculate the range for 10% variation\n",
    "            min_length = max(1, int(n * 0.9))  # ensure minimum length is at least 1\n",
    "            max_length = int(n * 1.1)\n",
    "            \n",
    "            # generate n-grams of varying lengths within the 10% range\n",
    "            n_grams = []\n",
    "            for length in range(min_length, max_length + 1):\n",
    "                n_grams.extend([' '.join(comparison_words[j:j+length]) for j in range(len(comparison_words) - length + 1)])\n",
    "            \n",
    "            # remove empty n-grams\n",
    "            n_grams = [ng for ng in n_grams if ng.strip()]\n",
    "\n",
    "            # find best match at current threshold\n",
    "            match = find_best_match(word_seg, n_grams, threshold)\n",
    "            \n",
    "            if match:\n",
    "                matched_text = match[0]\n",
    "                similarity = match[1]\n",
    "                \n",
    "                # remove the matched text from comparison_words\n",
    "                matched_words = matched_text.split()\n",
    "                start_index = None\n",
    "                for i in range(len(comparison_words) - len(matched_words) + 1):\n",
    "                    if comparison_words[i:i+len(matched_words)] == matched_words:\n",
    "                        start_index = i\n",
    "                        break\n",
    "                if start_index is None:\n",
    "                    # if no match is found, raise exception\n",
    "                    raise ValueError(\"Matched words not found in comparison_words\")\n",
    "                del comparison_words[start_index:start_index+len(matched_words)]                \n",
    "                             \n",
    "                # add mapping to output table           \n",
    "                mapping_table.append({\n",
    "                    'Segment Number': seg_num,\n",
    "                    'Baseline Segment': word_seg,\n",
    "                    'Matched Comparison Text': matched_text,\n",
    "                    'Similarity Score (%)': similarity\n",
    "                })\n",
    "            else:\n",
    "                still_unmatched.append((seg_num, word_seg))\n",
    "        \n",
    "        # keep track of unmatched segments for next iteration\n",
    "        unmatched_segments = still_unmatched\n",
    "\n",
    "    # add any remaining unmatched segments to mapping table\n",
    "    for seg_num, word_seg in unmatched_segments:\n",
    "        mapping_table.append({\n",
    "            'Segment Number': seg_num,\n",
    "            'Baseline Segment': word_seg,\n",
    "            'Matched Comparison Text': '',\n",
    "            'Similarity Score (%)': 0\n",
    "        })\n",
    "\n",
    "    # sort mapping table by segment number\n",
    "    mapping_table.sort(key=lambda x: x['Segment Number'])\n",
    "\n",
    "    print(\"\\nMatching summary:\\n\")\n",
    "    for threshold in similarity_thresholds + [0]:\n",
    "        count = sum(1 for mapping in mapping_table \n",
    "                   if (threshold == 0 and mapping['Similarity Score (%)'] == 0) or\n",
    "                   (0 < threshold <= mapping['Similarity Score (%)'] < (threshold + 10 if threshold < 90 else 101)))\n",
    "        if count > 0:\n",
    "            if threshold == 0:\n",
    "                print(f\"Segments with no match: {count}\")\n",
    "            else:\n",
    "                print(f\"Segments with {threshold}-{threshold+9}% similarity: {count}\")\n",
    "\n",
    "    # compute overall similarity score as weighted average across all segments, weighting by segment length\n",
    "    total_size = 0\n",
    "    total_weighted_similarity = 0\n",
    "    for mapping in mapping_table:\n",
    "        # add mapping to running totals\n",
    "        total_size += len(mapping['Baseline Segment'])\n",
    "        total_weighted_similarity += len(mapping['Baseline Segment']) * mapping['Similarity Score (%)']\n",
    "    # calculate final weighted score\n",
    "    overall_similarity = total_weighted_similarity / total_size if total_size > 0 else 0\n",
    "    \n",
    "    # output total similarity score\n",
    "    print(f\"\\nOverall Similarity Score: {overall_similarity:.2f}%\")\n",
    "    \n",
    "    # reconstruct comparison text without matched segments\n",
    "    remaining_comparison_text = ' '.join(word for word in comparison_words if word)\n",
    "    \n",
    "    # report and add unmatched text to mapping table\n",
    "    if remaining_comparison_text:\n",
    "        print(f\"\\nFound {len(remaining_comparison_text)} characters of unmatched text: {remaining_comparison_text}\")\n",
    "        mapping_table.append({\n",
    "            'Segment Number': f'99999',\n",
    "            'Baseline Segment': '',\n",
    "            'Matched Comparison Text': remaining_comparison_text,\n",
    "            'Similarity Score (%)': 0\n",
    "        })\n",
    "    \n",
    "    # export mapping table\n",
    "    if mapping_table:\n",
    "        df = pd.DataFrame(mapping_table)\n",
    "        output_table_path = os.path.join(output_dir, f\"{base_filename}-mapping-table.xlsx\")\n",
    "        try:\n",
    "            df.to_excel(output_table_path, index=False, engine='openpyxl')\n",
    "            print(f\"\\nMapping table has been exported to {output_table_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError exporting mapping table: {e}\")\n",
    "    else:\n",
    "        print(\"\\nNo mapping information to export.\")"
   ],
   "id": "fbece719602faf2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comparing Markdown documents\n",
    "\n",
    "The next several sections have code to compare the LLM and no-LLM versions of the Markdown outputs. They are a work-in-progress.\n",
    "\n",
    "Many thanks to Laterite for contributing ideas and code for this evaluation process.\n",
    "\n",
    "For each compared filename, this comparison process will output:\n",
    "\n",
    "1. **In the output window**: Summary results, including a weighted similarity score and \"extra text\" in the LLM version\n",
    "2. **filename-mapping-table.xlsx**: A mapping table showing the similarity of each segment in the LLM version to the no-LLM version, with extra text at the end\n",
    "3. **filename-baseline-normalized.txt**: The normalized text of the no-LLM version\n",
    "4. **filename-comparison-normalized.txt**: The normalized text of the LLM version"
   ],
   "id": "d1e02188ca9ac38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# loop through input files and compare the -no-llm version to the LLM version of the output files\n",
    "for filename in os.listdir(input_dir):\n",
    "    if os.path.isfile(os.path.join(input_dir, filename)) and not filename.startswith('.') and not filename.endswith('.md'):\n",
    "        output_path1 = os.path.join(output_dir, os.path.splitext(os.path.basename(filename))[0] + '.md')\n",
    "        output_path2 = os.path.join(output_dir, os.path.splitext(os.path.basename(filename))[0] + '-no-llm.md')\n",
    "\n",
    "        print(f\"\\n\\nComparing {output_path1} to {output_path2}...\")\n",
    "        print()\n",
    "        with open(output_path1, 'r') as f:\n",
    "            md1 = f.read()\n",
    "        with open(output_path2, 'r') as f:\n",
    "            md2 = f.read()\n",
    "\n",
    "        # compare using the Laterite method\n",
    "        compare_text(DocumentInterface.markdown_to_text(md2), DocumentInterface.markdown_to_text(md1), output_dir, os.path.splitext(os.path.basename(filename))[0])"
   ],
   "id": "cb34052874464dc9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
