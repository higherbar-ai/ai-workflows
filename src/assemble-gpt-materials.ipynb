{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ai_workflows custom GPT assembler\n",
    "\n",
    "This notebook assembles a system prompt and knowledge-base content for a custom GPT that helps people use the ai_workflows package. It's designed to run locally, within an ai_workflows development environment.\n",
    "\n",
    "It outputs the system prompt and knowledge base files to `~/ai-workflows/gpt-system-prompt.md` (where `~` is your user home directory).\n",
    "\n",
    "## Before running this\n",
    "\n",
    "1. Make sure the local documentation has been built (see README for details)\n",
    "2. If there are any new modules, make sure that they are added to the modules list in the code block below\n",
    "3. Configure your local `.ini` file as discussed below\n",
    "\n",
    "## Configuration\n",
    "\n",
    "The notebook begins by loading credentials and configuration from an `.ini` file stored in `~/.hbai/ai-workflows.ini`. The `~` in the path refers to the current user's home directory, and the `.ini` file contents should follow this format (with keys, models, and paths as appropriate):\n",
    "\n",
    "    [openai]\n",
    "    openai-api-key=keyhere-with-sk-on-front\n",
    "    openai-model=gpt-4o\n",
    "    azure-api-key=keyhere-or-blank\n",
    "    azure-api-base=azure-base-url-here\n",
    "    azure-api-engine=gpt-4o\n",
    "    azure-api-version=2024-02-01\n",
    "\n",
    "    [anthropic]\n",
    "    anthropic-api-key=keyhere\n",
    "    anthropic-model=\n",
    "\n",
    "    [aws]\n",
    "    aws-profile=\n",
    "    bedrock-model=\n",
    "    bedrock-region=us-east-1\n",
    "\n",
    "    [langsmith]\n",
    "    langsmith-api-key=leave-blank-unless-you're-using-langsmith\n",
    "\n",
    "You can set up either OpenAI, Azure, Anthropic, or Bedrock as the LLM, leaving settings for the other LLMs blank. You also don't need to supply a Langsmith API key unless you're using Langsmith.\n",
    "\n",
    "If you don't have an API key for an AI provider yet, [see here to learn what that is and how to get one](https://www.linkedin.com/pulse/those-genai-api-keys-christopher-robert-l5rie/).\n"
   ],
   "id": "a736b6304d0f881c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "modules = [\n",
    "    \"llm_utilities\",\n",
    "    \"document_utilities\",\n",
    "]\n",
    "\n",
    "# for convenience, auto-reload modules when they've changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "import configparser\n",
    "import os\n",
    "from ai_workflows.llm_utilities import LLMInterface\n",
    "from ai_workflows.document_utilities import DocumentInterface\n",
    "\n",
    "# set log level to WARNING\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# load credentials and other configuration from a local ini file\n",
    "inifile_location = os.path.expanduser(\"~/.hbai/ai-workflows.ini\")\n",
    "inifile = configparser.RawConfigParser()\n",
    "inifile.read(inifile_location)\n",
    "\n",
    "# load configuration\n",
    "openai_api_key = inifile.get(\"openai\", \"openai-api-key\")\n",
    "openai_model = inifile.get(\"openai\", \"openai-model\")\n",
    "azure_api_key = inifile.get(\"openai\", \"azure-api-key\")\n",
    "azure_api_base = inifile.get(\"openai\", \"azure-api-base\")\n",
    "azure_api_engine = inifile.get(\"openai\", \"azure-api-engine\")\n",
    "azure_api_version = inifile.get(\"openai\", \"azure-api-version\")\n",
    "anthropic_api_key = inifile.get(\"anthropic\", \"anthropic-api-key\")\n",
    "anthropic_model = inifile.get(\"anthropic\", \"anthropic-model\")\n",
    "aws_profile = inifile.get(\"aws\", \"aws-profile\")\n",
    "bedrock_model = inifile.get(\"aws\", \"bedrock-model\")\n",
    "bedrock_region = inifile.get(\"aws\", \"bedrock-region\")\n",
    "input_dir = os.path.expanduser(inifile.get(\"files\", \"input-dir\"))\n",
    "output_dir = os.path.expanduser(inifile.get(\"files\", \"output-dir\"))\n",
    "langsmith_api_key = inifile.get(\"langsmith\", \"langsmith-api-key\")\n",
    "\n",
    "# initialize the LLM\n",
    "llm = LLMInterface(\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_model=openai_model,\n",
    "    azure_api_key=azure_api_key,\n",
    "    azure_api_base=azure_api_base,\n",
    "    azure_api_engine=azure_api_engine,\n",
    "    azure_api_version=azure_api_version,\n",
    "    langsmith_api_key=langsmith_api_key,\n",
    "    anthropic_api_key=anthropic_api_key,\n",
    "    anthropic_model=anthropic_model,\n",
    "    bedrock_model=bedrock_model,\n",
    "    bedrock_region=bedrock_region,\n",
    "    bedrock_aws_profile=aws_profile,\n",
    "    max_tokens=16384                       # (note that we require a higher output token limit for this task)\n",
    ")\n",
    "\n",
    "# initialize our document processor (no LLM needed)\n",
    "doc_interface = DocumentInterface()\n",
    "\n",
    "# function to condense/clean Markdown for knowledge base\n",
    "def condense_for_kb(md):\n",
    "    prompt = f\"\"\"I am preparing documentation to ground an LLM-based coding assistant. This coding assistant is designed for people using the `ai_workflows` Python package. Following is some documentation converted from the original package's HTML documentation. Please revise this Markdown as follows:\n",
    "\n",
    "1. Remove anything about Sphinx, the ReadTheDocs theme, or anything else about how the documentation was produced\n",
    "2. Clean up the formatting, starting with a second-level heading (`##`) at the top\n",
    "\n",
    "Please retain all essential documentation, including examples, explanations, and hyperlinks. Please also return the revised Markdown without any enclosing code block or other formatting (just return the Markdown text alone).\n",
    "\n",
    "Here is the Markdown to revise, enclosed in |@| delimiters:\n",
    "|@|{md}|@|\n",
    "\n",
    "Your revised version: \"\"\"\n",
    "    return llm.get_llm_response(prompt)\n",
    "\n",
    "# function to condense/clean Markdown for system prompt\n",
    "def condense_for_system_prompt(md):\n",
    "    prompt = f\"\"\"I am preparing a system prompt to ground an LLM-based coding assistant. This coding assistant is designed for people using the `ai_workflows` Python package. Following is the overview converted from the original package's HTML documentation. Please revise this Markdown as follows:\n",
    "\n",
    "1. Remove anything about Sphinx, the ReadTheDocs theme, or anything else about how the documentation was produced\n",
    "2. Clean up the formatting, starting with a second-level heading (`##`) at the top\n",
    "3. Condense to include only the most essential details about installing and using the package, plus any core concepts that are important for users to understand\n",
    "4. Skip discussion of internal implementation details, but be sure to retain important guidance about how to specify JSON output specifications and work with JSON responses\n",
    "\n",
    "Please return the revised Markdown without any enclosing code block or other formatting (just return the Markdown text alone).\n",
    "\n",
    "Here is the Markdown to revise, enclosed in |@| delimiters:\n",
    "|@|{md}|@|\n",
    "\n",
    "Your revised version: \"\"\"\n",
    "    return llm.get_llm_response(prompt)\n",
    "\n",
    "# convert top-level docs page to Markdown for system prompt and knowledge base\n",
    "orig_index_md = doc_interface.convert_to_markdown(\"../docs/build/html/index.html\")\n",
    "index_filename = f\"kb_overview.md\"\n",
    "index_url = \"https://ai-workflows.readthedocs.io/en/latest/\"\n",
    "kb_file_list = f\"   - {index_filename}: overview documentation for the `ai_workflows` package (available at {index_url})\\n\"\n",
    "system_prompt_overview = condense_for_system_prompt(orig_index_md)\n",
    "index_md = f\"# ai_workflows overview documentation (available at {index_url}):\\n\\n\" + condense_for_kb(orig_index_md)\n",
    "output_path = os.path.expanduser(f\"~/ai-workflows/{index_filename}\")\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(index_md)\n",
    "print(f\"Wrote overview knowledge base item to {output_path}\")\n",
    "\n",
    "# assemble knowledge base\n",
    "for module in modules:\n",
    "    url=f\"https://ai-workflows.readthedocs.io/en/latest/ai_workflows.{module}.html\"\n",
    "    filename = f\"kb_{module}.md\"\n",
    "    kb_file_list += f\"   - {filename}: reference documentation for the `{module}` module (available at {url})\\n\"\n",
    "    module_md = f\"# {module} reference documentation (available at {url}):\\n\\n\" + condense_for_kb(doc_interface.convert_to_markdown(f\"../docs/build/html/ai_workflows.{module}.html\"))\n",
    "    output_path = os.path.expanduser(f\"~/ai-workflows/{filename}\")\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(module_md)\n",
    "    print(f\"Wrote {module} knowledge base item to {output_path}\")\n",
    "\n",
    "# construct system prompt\n",
    "system_prompt_md = f\"\"\"You are a coding assistant for users of the `ai_workflows` Python package. Many of these users might be new to Python or otherwise inexperienced (they might, for example, be a social scientist or a program evaluator who seeks to use `ai_workflows` to systematize a workflow with AI assistance). In assisting the user, consider the following:\n",
    "\n",
    "1. The condensed overview documentation included below\n",
    "2. The full reference documentation in the knowledge base, including the following files:\n",
    "{kb_file_list}\n",
    "# Condensed overview documentation enclosed in |@| delimiters (available at {index_url}):\n",
    "\n",
    "|@|\n",
    "{system_prompt_overview}\n",
    "|@|\n",
    "\n",
    "Additional instructions:\n",
    "\n",
    "1. **Prioritize `DocumentInterface` for Document-Based Workflows:**\n",
    "   - When the user’s task begins with files or document strings (e.g., PDFs, Word documents, Markdown), default to using `DocumentInterface` methods for processing. This includes:\n",
    "     - Converting files to Markdown (`convert_to_markdown()`).\n",
    "     - Extracting structured data from files or Markdown (`convert_to_json()` or `markdown_to_json()`).\n",
    "   - Avoid recommending standalone NLP libraries (e.g., spaCy, NLTK) for text extraction unless explicitly requested.\n",
    "\n",
    "2. **Prioritize `LLMInterface` for Direct LLM Interaction:**\n",
    "   - When the user needs to interact directly with an LLM (e.g., generating a response to a query or extracting structured data from arbitrary text), default to using `LLMInterface` methods, such as:\n",
    "     - `get_llm_response()` for free-form text outputs.\n",
    "     - `get_json_response()` for structured outputs.\n",
    "   - Emphasize the flexibility of `LLMInterface` methods for tasks unrelated to initial document processing, such as generating summaries or answering questions.\n",
    "\n",
    "3. **Assume Integration with LLMs:**\n",
    "   - For any structured information extraction or response generation task, assume the user wants to use LLM-based workflows. Avoid suggesting non-LLM solutions unless explicitly requested.\n",
    "\n",
    "4. **Default to JSON Outputs for Structured Data:**\n",
    "   - For tasks involving structured outputs (e.g., extracting names, lists, or entities), prioritize `convert_to_json()` or `markdown_to_json()` when starting from documents.\n",
    "   - Use `get_json_response()` when the input is not document-based.\n",
    "\n",
    "5. **Require LLM for All JSON Operations:**\n",
    "   - Remember that an `LLMInterface` is always required for the `DocumentInterface` methods that convert to JSON (because they use an LLM to convert to JSON).\n",
    "\n",
    "6. **Remember that `DocumentInterface` Methods Generate JSON in Batches:**\n",
    "   - Remember that `DocumentInterface` methods that convert to JSON (e.g., `convert_to_json()`, `markdown_to_json()`) generate JSON in batches. This means that the output is a list of JSON results, each following the structure specified in the `json_output_spec` parameter. The `merge_dicts()` method can be used to combine these results into a single dictionary.\n",
    "\n",
    "7. **Encourage Customization of Prompts:**\n",
    "   - Highlight the importance of customizing `json_context`, `json_job`, and `json_output_spec` parameters to tailor LLM behavior for the user’s needs.\n",
    "   - Provide specific examples of prompt customization for tasks like name extraction or question generation.\n",
    "\n",
    "8. **Be Concise, Specific, and Accurate**\n",
    "   - Unless instructed otherwise, always be concise, specific, and accurate in your responses.\n",
    "\n",
    "9. **Provide Hyperlinks**\n",
    "   - Always embed hyperlinks in every response where specific documentation, methods, or tools from `ai_workflows` are mentioned. Assume users will want clickable references for all methods and key concepts. Hyperlinks must be integrated inline whenever possible.\"\"\"\n",
    "\n",
    "# write to system prompt file\n",
    "output_path = os.path.expanduser(\"~/ai-workflows/gpt-system-prompt.md\")\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(system_prompt_md)\n",
    "print(f\"Wrote system prompt to {output_path}\")"
   ],
   "id": "32f4385f9fdd05c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## What to do next\n",
    "\n",
    "The code block above will report which files have been output. Use those files to update the GPT's system prompt and knowledge base."
   ],
   "id": "b39449b4e9254ecd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
